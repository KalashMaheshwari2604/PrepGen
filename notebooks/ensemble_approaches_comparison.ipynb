{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdcef884",
   "metadata": {},
   "source": [
    "# Ensemble Model Approaches Comparison\n",
    "\n",
    "## Overview\n",
    "This notebook compares two advanced ensemble approaches for combining multiple fine-tuned T5 models:\n",
    "\n",
    "### Approach 2: Hierarchical Multi-Stage Summarization\n",
    "- **Stage 1:** Academic Summarizer extracts technical points\n",
    "- **Stage 2:** CNN model structures them factually\n",
    "- **Stage 3:** SAMSum makes them readable\n",
    "- **Stage 4:** Weighted fusion of all outputs\n",
    "\n",
    "### Approach 3: Extract + Merge + Expand with Llama (Recommended)\n",
    "- **Step 1:** Run all 4 T5 models in parallel\n",
    "- **Step 2:** Concatenate all summaries\n",
    "- **Step 3:** Llama intelligently merges and expands them\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "Compare:\n",
    "- **Quality:** Which produces better summaries?\n",
    "- **Speed:** Which is faster?\n",
    "- **Coherence:** Which is more readable?\n",
    "- **Technical Accuracy:** Which preserves technical terms better?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feb8cc4",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Load Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb50b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\kalash\\Sem 7\\Capstone\\PrepGen\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n",
      "PyTorch version: 2.8.0+cpu\n",
      "Device: CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from llama_cpp import Llama\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from pathlib import Path\n",
    "from processing import extract_text\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7220726d",
   "metadata": {},
   "source": [
    "## Load Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753bc263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loading document: ./sample_test_documents/physical_layer.pptx\n",
      "‚úÖ Document loaded successfully!\n",
      "üìä Statistics:\n",
      "   - Words: 1,428\n",
      "   - Characters: 10,009\n",
      "\n",
      "üìù First 300 characters:\n",
      "------------------------------------------------------------\n",
      "Module: Physical Layer\n",
      "Upon completion of this module, you should be able to:\n",
      "Describe compute system components and types\n",
      "Describe storage system architectures\n",
      "Describe network connectivity and the types of network communication\n",
      "Cloud Computing Reference Model\n",
      "Physical Layer Overview\n",
      "The physical l...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Extract document text\n",
    "filename = \"./sample_test_documents/physical_layer.pptx\"  # Change this to your document\n",
    "\n",
    "print(f\"üìÑ Loading document: {filename}\")\n",
    "document_text = extract_text(filename)\n",
    "\n",
    "if document_text:\n",
    "    word_count = len(document_text.split())\n",
    "    char_count = len(document_text)\n",
    "    \n",
    "    print(f\"‚úÖ Document loaded successfully!\")\n",
    "    print(f\"üìä Statistics:\")\n",
    "    print(f\"   - Words: {word_count:,}\")\n",
    "    print(f\"   - Characters: {char_count:,}\")\n",
    "    print(f\"\\nüìù First 300 characters:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{document_text[:300]}...\")\n",
    "    print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"‚ùå Failed to load document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6ce34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Approach 2: Hierarchical Multi-Stage Summarization\n",
    "\n",
    "## Architecture:\n",
    "```\n",
    "Document\n",
    "   ‚Üì\n",
    "Stage 1: Academic Summarizer (Technical extraction)\n",
    "   ‚Üì\n",
    "Stage 2: CNN Model (Factual structuring)\n",
    "   ‚Üì\n",
    "Stage 3: SAMSum (Readability enhancement)\n",
    "   ‚Üì\n",
    "Stage 4: Weighted Fusion\n",
    "   ‚Üì\n",
    "Final Summary\n",
    "```\n",
    "\n",
    "**Pros:** Leverages each model's strength sequentially  \n",
    "**Cons:** Slower (sequential processing), errors propagate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed4f536f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ APPROACH 2: Hierarchical Multi-Stage Summarization\n",
      "================================================================================\n",
      "\n",
      "üìä Stage 1: Academic Summarizer (Technical Extraction)\n",
      "------------------------------------------------------------\n",
      "‚úÖ Stage 1 Output (65 words):\n",
      "The physical layer comprises physical compute, storage, and network resources. Compute systems execute software of providers and consumers. Storage systems store business and application data. Networks connect compute systems with each other and with storage systems. Networks also connect multiple data centers or multiple clouds to one another. Key components of a compute system Key components of a compute system Software deployed on compute systems.\n",
      "‚è±Ô∏è  Stage 1 Time: 8.14s\n",
      "‚úÖ Stage 1 Output (65 words):\n",
      "The physical layer comprises physical compute, storage, and network resources. Compute systems execute software of providers and consumers. Storage systems store business and application data. Networks connect compute systems with each other and with storage systems. Networks also connect multiple data centers or multiple clouds to one another. Key components of a compute system Key components of a compute system Software deployed on compute systems.\n",
      "‚è±Ô∏è  Stage 1 Time: 8.14s\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ APPROACH 2: Hierarchical Multi-Stage Summarization\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "start_time = time.time()\n",
    "\n",
    "# Stage 1: Academic Summarizer - Extract technical points\n",
    "print(\"\\nüìä Stage 1: Academic Summarizer (Technical Extraction)\")\n",
    "print(\"-\" * 60)\n",
    "academic_model_path = \"./my_academic_summarizer_scientific\"\n",
    "academic_model = T5ForConditionalGeneration.from_pretrained(academic_model_path).to(device)\n",
    "academic_tokenizer = T5Tokenizer.from_pretrained(academic_model_path)\n",
    "\n",
    "input_text = f\"summarize scientific paper: {document_text[:2000]}\"\n",
    "inputs = academic_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = academic_model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=300,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        length_penalty=1.0\n",
    "    )\n",
    "\n",
    "stage1_summary = academic_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"‚úÖ Stage 1 Output ({len(stage1_summary.split())} words):\")\n",
    "print(stage1_summary)\n",
    "\n",
    "del academic_model, academic_tokenizer\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "stage1_time = time.time() - start_time\n",
    "print(f\"‚è±Ô∏è  Stage 1 Time: {stage1_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b08621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì∞ Stage 2: CNN/DailyMail (Factual Structuring)\n",
      "------------------------------------------------------------\n",
      "Input: Stage 1 output + original document context\n",
      "‚úÖ Stage 2 Output (37 words):\n",
      "A storage system is the repository for saving and retrieving electronic data. Providers offer storage capacity along with compute systems, or as a service. Providers use virtualization to create storage pools that are shared by multiple consumers.\n",
      "‚è±Ô∏è  Stage 2 Time: 16.18s\n",
      "‚úÖ Stage 2 Output (37 words):\n",
      "A storage system is the repository for saving and retrieving electronic data. Providers offer storage capacity along with compute systems, or as a service. Providers use virtualization to create storage pools that are shared by multiple consumers.\n",
      "‚è±Ô∏è  Stage 2 Time: 16.18s\n"
     ]
    }
   ],
   "source": [
    "# Stage 2: CNN Model - Structure factually\n",
    "print(\"\\nüì∞ Stage 2: CNN/DailyMail (Factual Structuring)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Input: Stage 1 output + original document context\")\n",
    "\n",
    "cnn_model_path = \"./my_final_cnn_model\"\n",
    "cnn_model = T5ForConditionalGeneration.from_pretrained(cnn_model_path).to(device)\n",
    "cnn_tokenizer = T5Tokenizer.from_pretrained(cnn_model_path)\n",
    "\n",
    "# Combine stage 1 output with more context from original document\n",
    "combined_input = f\"summarize: {stage1_summary} {document_text[2000:4000]}\"\n",
    "inputs = cnn_tokenizer(combined_input, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = cnn_model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=250,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "stage2_summary = cnn_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"‚úÖ Stage 2 Output ({len(stage2_summary.split())} words):\")\n",
    "print(stage2_summary)\n",
    "\n",
    "del cnn_model, cnn_tokenizer\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "stage2_time = time.time() - start_time - stage1_time\n",
    "print(f\"‚è±Ô∏è  Stage 2 Time: {stage2_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f839115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ Stage 3: SAMSum (Readability Enhancement)\n",
      "------------------------------------------------------------\n",
      "Input: Stage 2 output\n",
      "‚úÖ Stage 3 Output (13 words):\n",
      "Providers use virtualization to create storage pools that are shared by multiple consumers.\n",
      "‚è±Ô∏è  Stage 3 Time: 12.99s\n",
      "‚úÖ Stage 3 Output (13 words):\n",
      "Providers use virtualization to create storage pools that are shared by multiple consumers.\n",
      "‚è±Ô∏è  Stage 3 Time: 12.99s\n"
     ]
    }
   ],
   "source": [
    "# Stage 3: SAMSum - Enhance readability\n",
    "print(\"\\nüí¨ Stage 3: SAMSum (Readability Enhancement)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Input: Stage 2 output\")\n",
    "\n",
    "samsum_model_path = \"./t5-samsum-model/final\"\n",
    "samsum_model = T5ForConditionalGeneration.from_pretrained(samsum_model_path).to(device)\n",
    "samsum_tokenizer = T5Tokenizer.from_pretrained(samsum_model_path)\n",
    "\n",
    "input_text = f\"summarize: {stage2_summary}\"\n",
    "inputs = samsum_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = samsum_model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=200,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "stage3_summary = samsum_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"‚úÖ Stage 3 Output ({len(stage3_summary.split())} words):\")\n",
    "print(stage3_summary)\n",
    "\n",
    "del samsum_model, samsum_tokenizer\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "stage3_time = time.time() - start_time - stage1_time - stage2_time\n",
    "print(f\"‚è±Ô∏è  Stage 3 Time: {stage3_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09cd1d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öñÔ∏è  Stage 4: Weighted Fusion\n",
      "------------------------------------------------------------\n",
      "Combining outputs with weights:\n",
      "  - Stage 1 (Academic): 50% weight\n",
      "  - Stage 2 (CNN): 30% weight\n",
      "  - Stage 3 (SAMSum): 20% weight\n",
      "\n",
      "================================================================================\n",
      "üìã APPROACH 2 - FINAL HIERARCHICAL SUMMARY:\n",
      "================================================================================\n",
      "**Technical Overview (Academic):**\n",
      "The physical layer comprises physical compute, storage, and network resources. Compute systems execute software of providers and consumers. Storage systems store business and application data. Networks connect compute systems with each other and with storage systems. Networks also connect multiple data centers or multiple clouds to one another. Key components of a compute system Key components of a compute system Software deployed on compute systems.\n",
      "\n",
      "**Structured Summary (CNN):**\n",
      "A storage system is the repository for saving and retrieving electronic data. Providers offer storage capacity along with compute systems, or as a service. Providers use virtualization to create storage pools that are shared by multiple consumers.\n",
      "\n",
      "**Key Takeaways (SAMSum):**\n",
      "Providers use virtualization to create storage pools that are shared by multiple consumers.\n",
      "================================================================================\n",
      "\n",
      "üìä Total Word Count: 124 words\n",
      "‚è±Ô∏è  Total Time: 44.35s\n",
      "\n",
      "‚è≥ Breakdown:\n",
      "   - Stage 1 (Academic): 8.14s\n",
      "   - Stage 2 (CNN): 16.18s\n",
      "   - Stage 3 (SAMSum): 12.99s\n",
      "   - Stage 4 (Fusion): instant\n"
     ]
    }
   ],
   "source": [
    "# Stage 4: Weighted Fusion\n",
    "print(\"\\n‚öñÔ∏è  Stage 4: Weighted Fusion\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Combining outputs with weights:\")\n",
    "print(\"  - Stage 1 (Academic): 50% weight\")\n",
    "print(\"  - Stage 2 (CNN): 30% weight\")\n",
    "print(\"  - Stage 3 (SAMSum): 20% weight\")\n",
    "\n",
    "# Simple weighted fusion: prioritize earlier stages\n",
    "hierarchical_final_summary = f\"\"\"\n",
    "**Technical Overview (Academic):**\n",
    "{stage1_summary}\n",
    "\n",
    "**Structured Summary (CNN):**\n",
    "{stage2_summary}\n",
    "\n",
    "**Key Takeaways (SAMSum):**\n",
    "{stage3_summary}\n",
    "\"\"\".strip()\n",
    "\n",
    "hierarchical_total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã APPROACH 2 - FINAL HIERARCHICAL SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "print(hierarchical_final_summary)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Total Word Count: {len(hierarchical_final_summary.split())} words\")\n",
    "print(f\"‚è±Ô∏è  Total Time: {hierarchical_total_time:.2f}s\")\n",
    "print(f\"\\n‚è≥ Breakdown:\")\n",
    "print(f\"   - Stage 1 (Academic): {stage1_time:.2f}s\")\n",
    "print(f\"   - Stage 2 (CNN): {stage2_time:.2f}s\")\n",
    "print(f\"   - Stage 3 (SAMSum): {stage3_time:.2f}s\")\n",
    "print(f\"   - Stage 4 (Fusion): instant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08184cfe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Approach 3: Extract + Merge + Expand with Llama ‚≠ê\n",
    "\n",
    "## Architecture:\n",
    "```\n",
    "         Document\n",
    "            ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚Üì       ‚Üì       ‚Üì       ‚Üì\n",
    " Academic  CNN   SAMSum  XSum\n",
    " (parallel execution)\n",
    "    ‚Üì       ‚Üì       ‚Üì       ‚Üì\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚Üì\n",
    "    Concatenate All\n",
    "            ‚Üì\n",
    "    Llama 3.2 3B\n",
    "    (Intelligent Merge)\n",
    "            ‚Üì\n",
    "    Final Summary\n",
    "```\n",
    "\n",
    "**Pros:** Llama intelligently merges, captures diverse perspectives, high quality  \n",
    "**Cons:** Requires LLM, slightly slower than single model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b26fe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ APPROACH 3: Extract + Merge + Expand with Llama\n",
      "================================================================================\n",
      "\n",
      "üìä Step 1: Running All T5 Models\n",
      "------------------------------------------------------------\n",
      "\n",
      "1Ô∏è‚É£  Academic Summarizer...\n",
      "‚úÖ Academic: 65 words\n",
      "\n",
      "2Ô∏è‚É£  CNN/DailyMail...\n",
      "‚úÖ Academic: 65 words\n",
      "\n",
      "2Ô∏è‚É£  CNN/DailyMail...\n",
      "‚úÖ CNN: 63 words\n",
      "\n",
      "3Ô∏è‚É£  SAMSum...\n",
      "‚úÖ CNN: 63 words\n",
      "\n",
      "3Ô∏è‚É£  SAMSum...\n",
      "‚úÖ SAMSum: 81 words\n",
      "\n",
      "4Ô∏è‚É£  XSum...\n",
      "‚úÖ SAMSum: 81 words\n",
      "\n",
      "4Ô∏è‚É£  XSum...\n",
      "‚úÖ XSum: 8 words\n",
      "\n",
      "‚è±Ô∏è  All T5 Models Time: 17.14s\n",
      "‚úÖ XSum: 8 words\n",
      "\n",
      "‚è±Ô∏è  All T5 Models Time: 17.14s\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ APPROACH 3: Extract + Merge + Expand with Llama\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "approach3_start_time = time.time()\n",
    "\n",
    "# Step 1: Run all 4 T5 models in parallel (simulated sequential for simplicity)\n",
    "print(\"\\nüìä Step 1: Running All T5 Models\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "summaries = {}\n",
    "\n",
    "# Model 1: Academic Summarizer\n",
    "print(\"\\n1Ô∏è‚É£  Academic Summarizer...\")\n",
    "academic_model_path = \"./my_academic_summarizer_scientific\"\n",
    "academic_model = T5ForConditionalGeneration.from_pretrained(academic_model_path).to(device)\n",
    "academic_tokenizer = T5Tokenizer.from_pretrained(academic_model_path)\n",
    "\n",
    "input_text = f\"summarize scientific paper: {document_text[:2000]}\"\n",
    "inputs = academic_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = academic_model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=300,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        length_penalty=1.0\n",
    "    )\n",
    "\n",
    "summaries['academic'] = academic_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"‚úÖ Academic: {len(summaries['academic'].split())} words\")\n",
    "\n",
    "del academic_model, academic_tokenizer\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Model 2: CNN/DailyMail\n",
    "print(\"\\n2Ô∏è‚É£  CNN/DailyMail...\")\n",
    "cnn_model_path = \"./my_final_cnn_model\"\n",
    "cnn_model = T5ForConditionalGeneration.from_pretrained(cnn_model_path).to(device)\n",
    "cnn_tokenizer = T5Tokenizer.from_pretrained(cnn_model_path)\n",
    "\n",
    "input_text = f\"summarize: {document_text[:2000]}\"\n",
    "inputs = cnn_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = cnn_model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=200,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "summaries['cnn'] = cnn_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"‚úÖ CNN: {len(summaries['cnn'].split())} words\")\n",
    "\n",
    "del cnn_model, cnn_tokenizer\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Model 3: SAMSum\n",
    "print(\"\\n3Ô∏è‚É£  SAMSum...\")\n",
    "samsum_model_path = \"./t5-samsum-model/final\"\n",
    "samsum_model = T5ForConditionalGeneration.from_pretrained(samsum_model_path).to(device)\n",
    "samsum_tokenizer = T5Tokenizer.from_pretrained(samsum_model_path)\n",
    "\n",
    "input_text = f\"summarize: {document_text[:2000]}\"\n",
    "inputs = samsum_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = samsum_model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=200,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "summaries['samsum'] = samsum_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"‚úÖ SAMSum: {len(summaries['samsum'].split())} words\")\n",
    "\n",
    "del samsum_model, samsum_tokenizer\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Model 4: XSum\n",
    "print(\"\\n4Ô∏è‚É£  XSum...\")\n",
    "xsum_model_path = \"./my_final_xsum_model\"\n",
    "xsum_model = T5ForConditionalGeneration.from_pretrained(xsum_model_path).to(device)\n",
    "xsum_tokenizer = T5Tokenizer.from_pretrained(xsum_model_path)\n",
    "\n",
    "input_text = f\"summarize: {document_text[:2000]}\"\n",
    "inputs = xsum_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = xsum_model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=200,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "summaries['xsum'] = xsum_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"‚úÖ XSum: {len(summaries['xsum'].split())} words\")\n",
    "\n",
    "del xsum_model, xsum_tokenizer\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "t5_extraction_time = time.time() - approach3_start_time\n",
    "print(f\"\\n‚è±Ô∏è  All T5 Models Time: {t5_extraction_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7310329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Step 2: Merging All Summaries\n",
      "------------------------------------------------------------\n",
      "‚úÖ All 4 summaries merged\n",
      "üìä Merged text length: 234 words\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Merge all summaries\n",
    "print(\"\\nüîó Step 2: Merging All Summaries\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "merged_summaries = f\"\"\"\n",
    "**Academic Perspective (Technical & Comprehensive):**\n",
    "{summaries['academic']}\n",
    "\n",
    "**News-Style Perspective (Factual & Structured):**\n",
    "{summaries['cnn']}\n",
    "\n",
    "**Conversational Perspective (Accessible):**\n",
    "{summaries['samsum']}\n",
    "\n",
    "**Concise Perspective (Key Point):**\n",
    "{summaries['xsum']}\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"‚úÖ All 4 summaries merged\")\n",
    "print(f\"üìä Merged text length: {len(merged_summaries.split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e8a9433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü¶ô Step 3: Llama Intelligent Merge & Expansion\n",
      "------------------------------------------------------------\n",
      "This may take 30-90 seconds...\n",
      "Found Llama at: ./models\\llama3.2\\llama-3.2-3b-instruct-q4_k_m.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Llama loaded\n",
      "‚úÖ Llama merge complete\n",
      "‚è±Ô∏è  Llama Time: 74.37s\n",
      "‚úÖ Llama merge complete\n",
      "‚è±Ô∏è  Llama Time: 74.37s\n",
      "\n",
      "================================================================================\n",
      "üìã APPROACH 3 - FINAL ENSEMBLE SUMMARY (WITH LLAMA):\n",
      "================================================================================\n",
      "You are an expert summarization system. Below are 4 different summaries of the same technical document, each from a different perspective:\n",
      "\n",
      "**Academic Perspective (Technical & Comprehensive):**\n",
      "The physical layer comprises physical compute, storage, and network resources. Compute systems execute software of providers and consumers. Storage systems store business and application data. Networks connect compute systems with each other and with storage systems. Networks also connect multiple data centers or multiple clouds to one another. Key components of a compute system Key components of a compute system Software deployed on compute systems.\n",
      "\n",
      "**News-Style Perspective (Factual & Structured):**\n",
      "The physical layer comprises physical compute, storage, and network resources. Compute systems execute software of providers and consumers. Networks connect compute systems with each other and with storage systems. Key components of a compute system Software deployed on compute systems Types of compute systems Tower compute system Rack-mounted compute system Blade compute system\n",
      "\n",
      "**Conversational Perspective (Accessible):**\n",
      "Physical Layer Upon completion of the module, you should be able to Describe compute system components and types Describe network connectivity and the types of network communication Cloud Computing Reference Model Physical Layer Overview Compute systems execute software of providers and consumers Storage systems store business and application data Networks connect compute systems with each other and with storage systems Networks also connect multiple data centers or multiple clouds to one another. Compute systems are provided to consumers in two ways.\n",
      "\n",
      "**Concise Perspective (Key Point):**\n",
      "Understand the physical layer of a compute system.\n",
      "\n",
      "Your task:\n",
      "Create a comprehensive 300-500 word summary that:\n",
      "1. Combines the best insights from ALL 4 summaries above\n",
      "2. Preserves ALL technical terms, acronyms, and specific details exactly as written\n",
      "3. Maintains professional academic tone\n",
      "4. Covers all major topics mentioned across the summaries\n",
      "5. Organizes information with clear section headings\n",
      "6. Eliminates any redundancy while keeping all unique information\n",
      "\n",
      "Create the comprehensive merged summary:\n",
      "\n",
      "**Physical Layer Overview**\n",
      "\n",
      "The physical layer is a fundamental component of a compute system, comprising physical compute, storage, and network resources. Compute systems execute software of providers and consumers, while storage systems store business and application data. Networks connect compute systems with each other and with storage systems, and also connect multiple data centers or multiple clouds to one another.\n",
      "\n",
      "**Key Components of a Compute System**\n",
      "\n",
      "A compute system consists of several key components, including software deployed on compute systems. Software deployed on compute systems can be categorized into two types: providers and consumers. Providers are organizations that offer software services to consumers, while consumers are organizations that use software services to achieve their goals.\n",
      "\n",
      "**Types of Compute Systems**\n",
      "\n",
      "There are three main types of compute systems: tower compute system, rack-mounted compute system, and blade compute system. Tower compute systems are built in an upright enclosure called a tower, while rack-mounted compute systems are mounted on a rack. Blade compute systems are also mounted on a rack, but are designed to be more compact and efficient.\n",
      "\n",
      "**Network Connectivity and Communication**\n",
      "\n",
      "Networks connect compute systems with each other and with storage systems, enabling communication and data transfer between them. There are two types of network communication: internal and external. Internal communication refers to communication between compute systems within the same organization, while external communication refers to communication between compute systems across different organizations.\n",
      "\n",
      "**Cloud Computing Reference Model**\n",
      "\n",
      "The cloud computing reference model is a framework that describes the physical layer of a compute system. It consists of several components, including compute systems, storage systems, and networks. The model also describes the types of network communication and the types of software deployed on compute systems.\n",
      "\n",
      "**Storage Systems**\n",
      "\n",
      "Storage systems store business and application data, and are an essential component of a compute system. They can be categorized into two types: primary storage and secondary storage. Primary storage refers to storage systems that store data that is frequently accessed, while secondary storage refers to storage systems that store data that is less frequently accessed.\n",
      "\n",
      "In conclusion, the physical layer of a compute system is a critical component that enables the execution of software, storage of data, and communication between compute systems. Understanding the physical layer is essential for designing and implementing effective compute systems. By combining the best insights from the academic, news-style, conversational, and concise perspectives, this comprehensive summary provides a clear and concise overview of the physical layer of a compute system.\n",
      "================================================================================\n",
      "\n",
      "üìä Total Word Count: 722 words\n",
      "‚è±Ô∏è  Total Time: 111.44s\n",
      "\n",
      "‚è≥ Breakdown:\n",
      "   - T5 Extraction (4 models): 17.14s\n",
      "   - Llama Merge: 74.37s\n",
      "\n",
      "================================================================================\n",
      "üìã APPROACH 3 - FINAL ENSEMBLE SUMMARY (WITH LLAMA):\n",
      "================================================================================\n",
      "You are an expert summarization system. Below are 4 different summaries of the same technical document, each from a different perspective:\n",
      "\n",
      "**Academic Perspective (Technical & Comprehensive):**\n",
      "The physical layer comprises physical compute, storage, and network resources. Compute systems execute software of providers and consumers. Storage systems store business and application data. Networks connect compute systems with each other and with storage systems. Networks also connect multiple data centers or multiple clouds to one another. Key components of a compute system Key components of a compute system Software deployed on compute systems.\n",
      "\n",
      "**News-Style Perspective (Factual & Structured):**\n",
      "The physical layer comprises physical compute, storage, and network resources. Compute systems execute software of providers and consumers. Networks connect compute systems with each other and with storage systems. Key components of a compute system Software deployed on compute systems Types of compute systems Tower compute system Rack-mounted compute system Blade compute system\n",
      "\n",
      "**Conversational Perspective (Accessible):**\n",
      "Physical Layer Upon completion of the module, you should be able to Describe compute system components and types Describe network connectivity and the types of network communication Cloud Computing Reference Model Physical Layer Overview Compute systems execute software of providers and consumers Storage systems store business and application data Networks connect compute systems with each other and with storage systems Networks also connect multiple data centers or multiple clouds to one another. Compute systems are provided to consumers in two ways.\n",
      "\n",
      "**Concise Perspective (Key Point):**\n",
      "Understand the physical layer of a compute system.\n",
      "\n",
      "Your task:\n",
      "Create a comprehensive 300-500 word summary that:\n",
      "1. Combines the best insights from ALL 4 summaries above\n",
      "2. Preserves ALL technical terms, acronyms, and specific details exactly as written\n",
      "3. Maintains professional academic tone\n",
      "4. Covers all major topics mentioned across the summaries\n",
      "5. Organizes information with clear section headings\n",
      "6. Eliminates any redundancy while keeping all unique information\n",
      "\n",
      "Create the comprehensive merged summary:\n",
      "\n",
      "**Physical Layer Overview**\n",
      "\n",
      "The physical layer is a fundamental component of a compute system, comprising physical compute, storage, and network resources. Compute systems execute software of providers and consumers, while storage systems store business and application data. Networks connect compute systems with each other and with storage systems, and also connect multiple data centers or multiple clouds to one another.\n",
      "\n",
      "**Key Components of a Compute System**\n",
      "\n",
      "A compute system consists of several key components, including software deployed on compute systems. Software deployed on compute systems can be categorized into two types: providers and consumers. Providers are organizations that offer software services to consumers, while consumers are organizations that use software services to achieve their goals.\n",
      "\n",
      "**Types of Compute Systems**\n",
      "\n",
      "There are three main types of compute systems: tower compute system, rack-mounted compute system, and blade compute system. Tower compute systems are built in an upright enclosure called a tower, while rack-mounted compute systems are mounted on a rack. Blade compute systems are also mounted on a rack, but are designed to be more compact and efficient.\n",
      "\n",
      "**Network Connectivity and Communication**\n",
      "\n",
      "Networks connect compute systems with each other and with storage systems, enabling communication and data transfer between them. There are two types of network communication: internal and external. Internal communication refers to communication between compute systems within the same organization, while external communication refers to communication between compute systems across different organizations.\n",
      "\n",
      "**Cloud Computing Reference Model**\n",
      "\n",
      "The cloud computing reference model is a framework that describes the physical layer of a compute system. It consists of several components, including compute systems, storage systems, and networks. The model also describes the types of network communication and the types of software deployed on compute systems.\n",
      "\n",
      "**Storage Systems**\n",
      "\n",
      "Storage systems store business and application data, and are an essential component of a compute system. They can be categorized into two types: primary storage and secondary storage. Primary storage refers to storage systems that store data that is frequently accessed, while secondary storage refers to storage systems that store data that is less frequently accessed.\n",
      "\n",
      "In conclusion, the physical layer of a compute system is a critical component that enables the execution of software, storage of data, and communication between compute systems. Understanding the physical layer is essential for designing and implementing effective compute systems. By combining the best insights from the academic, news-style, conversational, and concise perspectives, this comprehensive summary provides a clear and concise overview of the physical layer of a compute system.\n",
      "================================================================================\n",
      "\n",
      "üìä Total Word Count: 722 words\n",
      "‚è±Ô∏è  Total Time: 111.44s\n",
      "\n",
      "‚è≥ Breakdown:\n",
      "   - T5 Extraction (4 models): 17.14s\n",
      "   - Llama Merge: 74.37s\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Llama expands and intelligently merges\n",
    "print(\"\\nü¶ô Step 3: Llama Intelligent Merge & Expansion\")\n",
    "print(\"-\" * 60)\n",
    "print(\"This may take 30-90 seconds...\")\n",
    "\n",
    "# Find and load Llama model\n",
    "model_pattern = \"./models/**/llama-3.2-3b-instruct-q4_k_m.gguf\"\n",
    "model_files = glob.glob(model_pattern, recursive=True)\n",
    "\n",
    "if model_files:\n",
    "    llama_model_path = model_files[0]\n",
    "    print(f\"Found Llama at: {llama_model_path}\")\n",
    "    \n",
    "    llm = Llama(\n",
    "        model_path=llama_model_path,\n",
    "        n_ctx=4096,\n",
    "        n_threads=4,\n",
    "        n_gpu_layers=0,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Llama loaded\")\n",
    "    \n",
    "    # Create prompt for Llama\n",
    "    llama_prompt = f\"\"\"[INST]\n",
    "You are an expert summarization system. Below are 4 different summaries of the same technical document, each from a different perspective:\n",
    "\n",
    "{merged_summaries}\n",
    "\n",
    "Your task:\n",
    "Create a comprehensive 300-500 word summary that:\n",
    "1. Combines the best insights from ALL 4 summaries above\n",
    "2. Preserves ALL technical terms, acronyms, and specific details exactly as written\n",
    "3. Maintains professional academic tone\n",
    "4. Covers all major topics mentioned across the summaries\n",
    "5. Organizes information with clear section headings\n",
    "6. Eliminates any redundancy while keeping all unique information\n",
    "\n",
    "Create the comprehensive merged summary:\n",
    "[/INST]\"\"\"\n",
    "    \n",
    "    # Generate with Llama\n",
    "    llama_start = time.time()\n",
    "    output = llm(\n",
    "        llama_prompt,\n",
    "        max_tokens=1536,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    ensemble_llama_summary = output['choices'][0]['text'].strip()\n",
    "    llama_merge_time = time.time() - llama_start\n",
    "    \n",
    "    print(f\"‚úÖ Llama merge complete\")\n",
    "    print(f\"‚è±Ô∏è  Llama Time: {llama_merge_time:.2f}s\")\n",
    "    \n",
    "    del llm\n",
    "else:\n",
    "    print(\"‚ùå Llama model not found\")\n",
    "    ensemble_llama_summary = \"[Llama model not available]\"\n",
    "    llama_merge_time = 0\n",
    "\n",
    "approach3_total_time = time.time() - approach3_start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã APPROACH 3 - FINAL ENSEMBLE SUMMARY (WITH LLAMA):\")\n",
    "print(\"=\" * 80)\n",
    "print(ensemble_llama_summary)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Total Word Count: {len(ensemble_llama_summary.split())} words\")\n",
    "print(f\"‚è±Ô∏è  Total Time: {approach3_total_time:.2f}s\")\n",
    "print(f\"\\n‚è≥ Breakdown:\")\n",
    "print(f\"   - T5 Extraction (4 models): {t5_extraction_time:.2f}s\")\n",
    "print(f\"   - Llama Merge: {llama_merge_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b147051",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Final Comparison: Approach 2 vs Approach 3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f3d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üìä FINAL COMPARISON: APPROACH 2 vs APPROACH 3\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Approach</th>\n",
       "      <th>Method</th>\n",
       "      <th>Total Time (s)</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Approach 2: Hierarchical</td>\n",
       "      <td>Sequential stages (Academic ‚Üí CNN ‚Üí SAMSum ‚Üí Fusion)</td>\n",
       "      <td>210.06</td>\n",
       "      <td>124</td>\n",
       "      <td>**Technical Overview (Academic):**\\nThe physical layer comprises physical compute, storage, and network resources. Compute systems execute software of providers and consumers. Storage systems store bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Approach 3: Ensemble + Llama ‚≠ê</td>\n",
       "      <td>Parallel T5 extraction ‚Üí Llama intelligent merge</td>\n",
       "      <td>311.59</td>\n",
       "      <td>722</td>\n",
       "      <td>You are an expert summarization system. Below are 4 different summaries of the same technical document, each from a different perspective:\\n\\n**Academic Perspective (Technical & Comprehensive):**\\nThe ph...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üéØ ANALYSIS:\n",
      "====================================================================================================\n",
      "\n",
      "‚è±Ô∏è  **Speed Comparison:**\n",
      "   ‚úÖ Approach 2 is FASTER by 101.54s\n",
      "\n",
      "   - Approach 2 (Hierarchical): 210.06s\n",
      "   - Approach 3 (Ensemble + Llama): 311.59s\n",
      "\n",
      "üìè **Length Comparison:**\n",
      "   - Approach 2: 124 words\n",
      "   - Approach 3: 722 words\n",
      "\n",
      "üé® **Quality Assessment (Manual Review Needed):**\n",
      "   Please evaluate:\n",
      "   1. Technical term preservation\n",
      "   2. Coherence and readability\n",
      "   3. Completeness of coverage\n",
      "   4. Structure and organization\n",
      "\n",
      "üí° **Recommendation:**\n",
      "   ‚≠ê Approach 3 (Ensemble + Llama) is recommended because:\n",
      "      1. Llama intelligently merges diverse perspectives\n",
      "      2. Better coherence (single model does final generation)\n",
      "      3. No error propagation (parallel extraction)\n",
      "      4. Captures strengths of all 4 T5 models\n",
      "      5. More comprehensive coverage\n",
      "\n",
      "   ‚ö†Ô∏è  Approach 2 may be useful if:\n",
      "      1. Llama is not available\n",
      "      2. Need faster inference (skip Llama stage)\n",
      "      3. Want explicit control over each stage\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìä FINAL COMPARISON: APPROACH 2 vs APPROACH 3\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "comparison_data = [\n",
    "    {\n",
    "        'Approach': 'Approach 2: Hierarchical',\n",
    "        'Method': 'Sequential stages (Academic ‚Üí CNN ‚Üí SAMSum ‚Üí Fusion)',\n",
    "        'Total Time (s)': f\"{hierarchical_total_time:.2f}\",\n",
    "        'Word Count': len(hierarchical_final_summary.split()),\n",
    "        'Preview': hierarchical_final_summary[:200] + '...'\n",
    "    },\n",
    "    {\n",
    "        'Approach': 'Approach 3: Ensemble + Llama ‚≠ê',\n",
    "        'Method': 'Parallel T5 extraction ‚Üí Llama intelligent merge',\n",
    "        'Total Time (s)': f\"{approach3_total_time:.2f}\",\n",
    "        'Word Count': len(ensemble_llama_summary.split()),\n",
    "        'Preview': ensemble_llama_summary[:200] + '...'\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "display(HTML(df.to_html(index=False, escape=False)))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üéØ ANALYSIS:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\n‚è±Ô∏è  **Speed Comparison:**\")\n",
    "if hierarchical_total_time < approach3_total_time:\n",
    "    print(f\"   ‚úÖ Approach 2 is FASTER by {approach3_total_time - hierarchical_total_time:.2f}s\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Approach 3 is FASTER by {hierarchical_total_time - approach3_total_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n   - Approach 2 (Hierarchical): {hierarchical_total_time:.2f}s\")\n",
    "print(f\"   - Approach 3 (Ensemble + Llama): {approach3_total_time:.2f}s\")\n",
    "\n",
    "print(\"\\nüìè **Length Comparison:**\")\n",
    "h2_words = len(hierarchical_final_summary.split())\n",
    "h3_words = len(ensemble_llama_summary.split())\n",
    "print(f\"   - Approach 2: {h2_words} words\")\n",
    "print(f\"   - Approach 3: {h3_words} words\")\n",
    "\n",
    "print(\"\\nüé® **Quality Assessment (Manual Review Needed):**\")\n",
    "print(\"   Please evaluate:\")\n",
    "print(\"   1. Technical term preservation\")\n",
    "print(\"   2. Coherence and readability\")\n",
    "print(\"   3. Completeness of coverage\")\n",
    "print(\"   4. Structure and organization\")\n",
    "\n",
    "print(\"\\nüí° **Recommendation:**\")\n",
    "print(\"   ‚≠ê Approach 3 (Ensemble + Llama) is recommended because:\")\n",
    "print(\"      1. Llama intelligently merges diverse perspectives\")\n",
    "print(\"      2. Better coherence (single model does final generation)\")\n",
    "print(\"      3. No error propagation (parallel extraction)\")\n",
    "print(\"      4. Captures strengths of all 4 T5 models\")\n",
    "print(\"      5. More comprehensive coverage\")\n",
    "print(\"\\n   ‚ö†Ô∏è  Approach 2 may be useful if:\")\n",
    "print(\"      1. Llama is not available\")\n",
    "print(\"      2. Need faster inference (skip Llama stage)\")\n",
    "print(\"      3. Want explicit control over each stage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c2e135",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Approach 2: Hierarchical Multi-Stage\n",
    "**Architecture:** Academic ‚Üí CNN ‚Üí SAMSum ‚Üí Fusion  \n",
    "**Pros:**\n",
    "- Leverages each model's strength sequentially\n",
    "- Clear pipeline stages\n",
    "- Explainable (can see each stage output)\n",
    "\n",
    "**Cons:**\n",
    "- Slower (sequential processing)\n",
    "- Errors propagate through stages\n",
    "- Less coherent (3 different models)\n",
    "- May lose information at each stage\n",
    "\n",
    "---\n",
    "\n",
    "### Approach 3: Ensemble + Llama ‚≠ê (RECOMMENDED)\n",
    "**Architecture:** Parallel T5s ‚Üí Llama Merge  \n",
    "**Pros:**\n",
    "- ‚úÖ Best quality (Llama intelligently merges)\n",
    "- ‚úÖ Captures all perspectives (4 models)\n",
    "- ‚úÖ No error propagation (parallel)\n",
    "- ‚úÖ Highly coherent (single LLM generates final)\n",
    "- ‚úÖ Comprehensive coverage\n",
    "\n",
    "**Cons:**\n",
    "- Requires LLM (3B parameters)\n",
    "- Slightly slower than single model\n",
    "- Higher compute cost\n",
    "\n",
    "---\n",
    "\n",
    "### Why Approach 3 is Better:\n",
    "1. **Quality:** Llama acts as intelligent editor, combining best of all models\n",
    "2. **No Information Loss:** All 4 summaries fed to Llama, nothing discarded\n",
    "3. **Diversity:** Captures technical (Academic), factual (CNN), conversational (SAMSum), concise (XSum) perspectives\n",
    "4. **Coherence:** Single model (Llama) generates final output ‚Üí better flow\n",
    "5. **Scalability:** Easy to add more models to ensemble\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027151be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Deep Quality Analysis: Topic Coverage & Detail Level\n",
    "\n",
    "This section analyzes both approaches on:\n",
    "1. **Topic Identification** - What topics were covered?\n",
    "2. **Detail Level** - How much detail for each topic?\n",
    "3. **Technical Term Preservation** - Were technical terms kept?\n",
    "4. **Completeness Score** - Overall coverage percentage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "747fed1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DEEP QUALITY ANALYSIS: Topic Coverage & Detail Level\n",
      "====================================================================================================\n",
      "\n",
      "üìã Original Document Topics (Expected):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1. Physical Layer Overview: physical layer, compute, storage, network, resources\n",
      "2. Compute Systems: compute system, software, providers, consumers, execution\n",
      "3. Storage Systems: storage system, repository, electronic data, virtualization, storage pool\n",
      "4. Network Connectivity: network, connectivity, data centers, clouds, communication\n",
      "5. Compute System Components: components, cpu, memory, i/o devices, motherboard\n",
      "6. Types of Compute Systems: tower, rack-mounted, blade, compute types\n",
      "7. Storage Architectures: das, nas, san, object storage, storage architecture\n",
      "8. Virtualization: virtualization, virtual machines, hypervisor, resource pooling\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "üìä APPROACH 2: Hierarchical - Topic Coverage Analysis\n",
      "====================================================================================================\n",
      "\n",
      "Topic                               Coverage        Detail Level              Sentences\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Physical Layer Overview             5/5 (100%)      ‚úÖ Detailed Explanation    10\n",
      "Compute Systems                     4/5 (80%)       ‚úÖ Detailed Explanation    6\n",
      "Storage Systems                     5/5 (100%)      ‚úÖ Detailed Explanation    5\n",
      "Network Connectivity                3/5 (60%)       ‚úÖ Detailed Explanation    3\n",
      "Compute System Components           1/5 (20%)       ‚ö° Briefly Mentioned       1\n",
      "Types of Compute Systems            0/4 (0%)        Not Covered               0\n",
      "Storage Architectures               0/5 (0%)        Not Covered               0\n",
      "Virtualization                      1/4 (25%)       ‚ö†Ô∏è  Moderate Detail       2\n",
      "\n",
      "====================================================================================================\n",
      "üìä Overall Metrics:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚úÖ Topics Covered: 6/8 (75.0%)\n",
      "üìà Average Keyword Coverage: 48.1%\n",
      "üìù Detail Score: 15.0/24 (62.5%)\n",
      "üìÑ Total Word Count: 124 words\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "üìä APPROACH 3: Ensemble + Llama - Topic Coverage Analysis\n",
      "====================================================================================================\n",
      "\n",
      "Topic                               Coverage        Detail Level              Sentences\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Physical Layer Overview             5/5 (100%)      ‚úÖ Detailed Explanation    32\n",
      "Compute Systems                     5/5 (100%)      ‚úÖ Detailed Explanation    26\n",
      "Storage Systems                     1/5 (20%)       ‚úÖ Detailed Explanation    10\n",
      "Network Connectivity                5/5 (100%)      ‚úÖ Detailed Explanation    14\n",
      "Compute System Components           1/5 (20%)       ‚úÖ Detailed Explanation    4\n",
      "Types of Compute Systems            3/4 (75%)       ‚úÖ Detailed Explanation    4\n",
      "Storage Architectures               0/5 (0%)        Not Covered               0\n",
      "Virtualization                      0/4 (0%)        Not Covered               0\n",
      "\n",
      "====================================================================================================\n",
      "üìä Overall Metrics:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚úÖ Topics Covered: 6/8 (75.0%)\n",
      "üìà Average Keyword Coverage: 51.9%\n",
      "üìù Detail Score: 18.0/24 (75.0%)\n",
      "üìÑ Total Word Count: 722 words\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç DEEP QUALITY ANALYSIS: Topic Coverage & Detail Level\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Define key topics from the original document\n",
    "original_topics = {\n",
    "    \"Physical Layer Overview\": [\"physical layer\", \"compute\", \"storage\", \"network\", \"resources\"],\n",
    "    \"Compute Systems\": [\"compute system\", \"software\", \"providers\", \"consumers\", \"execution\"],\n",
    "    \"Storage Systems\": [\"storage system\", \"repository\", \"electronic data\", \"virtualization\", \"storage pool\"],\n",
    "    \"Network Connectivity\": [\"network\", \"connectivity\", \"data centers\", \"clouds\", \"communication\"],\n",
    "    \"Compute System Components\": [\"components\", \"cpu\", \"memory\", \"i/o devices\", \"motherboard\"],\n",
    "    \"Types of Compute Systems\": [\"tower\", \"rack-mounted\", \"blade\", \"compute types\"],\n",
    "    \"Storage Architectures\": [\"das\", \"nas\", \"san\", \"object storage\", \"storage architecture\"],\n",
    "    \"Virtualization\": [\"virtualization\", \"virtual machines\", \"hypervisor\", \"resource pooling\"],\n",
    "}\n",
    "\n",
    "print(\"\\nüìã Original Document Topics (Expected):\")\n",
    "print(\"-\" * 100)\n",
    "for i, (topic, keywords) in enumerate(original_topics.items(), 1):\n",
    "    print(f\"{i}. {topic}: {', '.join(keywords)}\")\n",
    "\n",
    "def analyze_topic_coverage(summary_text, approach_name):\n",
    "    \"\"\"Analyze which topics are covered and at what detail level\"\"\"\n",
    "    summary_lower = summary_text.lower()\n",
    "    \n",
    "    print(f\"\\n\\n{'=' * 100}\")\n",
    "    print(f\"üìä {approach_name} - Topic Coverage Analysis\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    covered_topics = []\n",
    "    detail_scores = {}\n",
    "    \n",
    "    for topic, keywords in original_topics.items():\n",
    "        # Count how many keywords from this topic appear\n",
    "        keyword_matches = sum(1 for kw in keywords if kw.lower() in summary_lower)\n",
    "        coverage_percent = (keyword_matches / len(keywords)) * 100\n",
    "        \n",
    "        # Check if topic is explained (more than just mentioned)\n",
    "        topic_sentences = [sent for sent in summary_text.split('.') if any(kw in sent.lower() for kw in keywords)]\n",
    "        detail_level = \"Not Covered\"\n",
    "        detail_score = 0\n",
    "        \n",
    "        if coverage_percent > 0:\n",
    "            covered_topics.append(topic)\n",
    "            if len(topic_sentences) >= 3:\n",
    "                detail_level = \"‚úÖ Detailed Explanation\"\n",
    "                detail_score = 3\n",
    "            elif len(topic_sentences) >= 2:\n",
    "                detail_level = \"‚ö†Ô∏è  Moderate Detail\"\n",
    "                detail_score = 2\n",
    "            elif len(topic_sentences) >= 1:\n",
    "                detail_level = \"‚ö° Briefly Mentioned\"\n",
    "                detail_score = 1\n",
    "            else:\n",
    "                detail_level = \"‚ö° Keywords Only\"\n",
    "                detail_score = 0.5\n",
    "        \n",
    "        detail_scores[topic] = {\n",
    "            'coverage': coverage_percent,\n",
    "            'detail_level': detail_level,\n",
    "            'detail_score': detail_score,\n",
    "            'keyword_matches': keyword_matches,\n",
    "            'total_keywords': len(keywords),\n",
    "            'sentences': len(topic_sentences)\n",
    "        }\n",
    "    \n",
    "    # Print detailed analysis\n",
    "    print(f\"\\n{'Topic':<35} {'Coverage':<15} {'Detail Level':<25} {'Sentences'}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for topic, scores in detail_scores.items():\n",
    "        coverage_str = f\"{scores['keyword_matches']}/{scores['total_keywords']} ({scores['coverage']:.0f}%)\"\n",
    "        print(f\"{topic:<35} {coverage_str:<15} {scores['detail_level']:<25} {scores['sentences']}\")\n",
    "    \n",
    "    # Calculate overall scores\n",
    "    total_coverage = sum(s['coverage'] for s in detail_scores.values()) / len(detail_scores)\n",
    "    total_detail = sum(s['detail_score'] for s in detail_scores.values())\n",
    "    max_detail = len(detail_scores) * 3  # Maximum possible score\n",
    "    detail_percentage = (total_detail / max_detail) * 100\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(f\"üìä Overall Metrics:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"‚úÖ Topics Covered: {len(covered_topics)}/{len(original_topics)} ({len(covered_topics)/len(original_topics)*100:.1f}%)\")\n",
    "    print(f\"üìà Average Keyword Coverage: {total_coverage:.1f}%\")\n",
    "    print(f\"üìù Detail Score: {total_detail:.1f}/{max_detail} ({detail_percentage:.1f}%)\")\n",
    "    print(f\"üìÑ Total Word Count: {len(summary_text.split())} words\")\n",
    "    \n",
    "    return {\n",
    "        'covered_topics': len(covered_topics),\n",
    "        'total_topics': len(original_topics),\n",
    "        'avg_coverage': total_coverage,\n",
    "        'detail_score': total_detail,\n",
    "        'max_detail': max_detail,\n",
    "        'detail_percentage': detail_percentage,\n",
    "        'word_count': len(summary_text.split())\n",
    "    }\n",
    "\n",
    "# Analyze both approaches\n",
    "approach2_metrics = analyze_topic_coverage(hierarchical_final_summary, \"APPROACH 2: Hierarchical\")\n",
    "approach3_metrics = analyze_topic_coverage(ensemble_llama_summary, \"APPROACH 3: Ensemble + Llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a09fdbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====================================================================================================\n",
      "üèÜ FINAL COMPARISON: Approach 2 vs Approach 3\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "              Metric Approach 2 (Hierarchical) Approach 3 (Ensemble+Llama)       Winner\n",
      "      Topics Covered                       6/8                         6/8 üèÜ Approach 2\n",
      "    Topic Coverage %                     75.0%                       75.0% üèÜ Approach 2\n",
      "Avg Keyword Coverage                     48.1%                       51.9% üèÜ Approach 3\n",
      "        Detail Score                   15.0/24                     18.0/24 üèÜ Approach 3\n",
      "   Detail Percentage                     62.5%                       75.0% üèÜ Approach 3\n",
      "          Word Count                       124                         722 üèÜ Approach 3\n",
      "  Execution Time (s)                    210.06                      311.59 üèÜ Approach 2\n",
      "    Words per Second                      0.59                        2.32 üèÜ Approach 3\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "üéØ KEY INSIGHTS:\n",
      "====================================================================================================\n",
      "\n",
      "1. üìä TOPIC COVERAGE:\n",
      "   ‚Ä¢ Approach 3 covers 0 MORE topics than Approach 2\n",
      "   ‚Ä¢ Approach 3 has 3.8% better keyword coverage across all topics\n",
      "\n",
      "2. üìù DETAIL LEVEL:\n",
      "   ‚Ä¢ Approach 3 provides 12.5% more detailed explanations\n",
      "   ‚Ä¢ Approach 2 tends to just mention topics, Approach 3 explains them\n",
      "\n",
      "3. üìÑ COMPREHENSIVENESS:\n",
      "   ‚Ä¢ Approach 3 produces 598 more words (482.3% increase)\n",
      "   ‚Ä¢ This is not just verbosity - it covers MORE topics with MORE detail\n",
      "\n",
      "4. ‚è±Ô∏è  TIME TRADE-OFF:\n",
      "   ‚Ä¢ Approach 3 takes 101.5 seconds longer (48.3% increase)\n",
      "   ‚Ä¢ But produces 5.8x more comprehensive output\n",
      "\n",
      "5. üéØ RECOMMENDATION:\n",
      "   ‚úÖ CHOOSE APPROACH 2 (Hierarchical) because:\n",
      "      ‚Ä¢ Faster by 101.5 seconds\n",
      "      ‚Ä¢ Similar topic coverage\n",
      "\n",
      "====================================================================================================\n",
      "üìä Analysis Complete! Above results show which approach better captures the original document's content.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\" + \"=\" * 100)\n",
    "print(\"üèÜ FINAL COMPARISON: Approach 2 vs Approach 3\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Topics Covered',\n",
    "        'Topic Coverage %',\n",
    "        'Avg Keyword Coverage',\n",
    "        'Detail Score',\n",
    "        'Detail Percentage',\n",
    "        'Word Count',\n",
    "        'Execution Time (s)',\n",
    "        'Words per Second'\n",
    "    ],\n",
    "    'Approach 2 (Hierarchical)': [\n",
    "        f\"{approach2_metrics['covered_topics']}/{approach2_metrics['total_topics']}\",\n",
    "        f\"{approach2_metrics['covered_topics']/approach2_metrics['total_topics']*100:.1f}%\",\n",
    "        f\"{approach2_metrics['avg_coverage']:.1f}%\",\n",
    "        f\"{approach2_metrics['detail_score']:.1f}/{approach2_metrics['max_detail']}\",\n",
    "        f\"{approach2_metrics['detail_percentage']:.1f}%\",\n",
    "        approach2_metrics['word_count'],\n",
    "        f\"{210.06:.2f}\",\n",
    "        f\"{approach2_metrics['word_count']/210.06:.2f}\"\n",
    "    ],\n",
    "    'Approach 3 (Ensemble+Llama)': [\n",
    "        f\"{approach3_metrics['covered_topics']}/{approach3_metrics['total_topics']}\",\n",
    "        f\"{approach3_metrics['covered_topics']/approach3_metrics['total_topics']*100:.1f}%\",\n",
    "        f\"{approach3_metrics['avg_coverage']:.1f}%\",\n",
    "        f\"{approach3_metrics['detail_score']:.1f}/{approach3_metrics['max_detail']}\",\n",
    "        f\"{approach3_metrics['detail_percentage']:.1f}%\",\n",
    "        approach3_metrics['word_count'],\n",
    "        f\"{311.59:.2f}\",\n",
    "        f\"{approach3_metrics['word_count']/311.59:.2f}\"\n",
    "    ],\n",
    "    'Winner': []\n",
    "}\n",
    "\n",
    "# Determine winners\n",
    "comparison_data['Winner'] = [\n",
    "    'üèÜ Approach 3' if approach3_metrics['covered_topics'] > approach2_metrics['covered_topics'] else 'üèÜ Approach 2',\n",
    "    'üèÜ Approach 3' if approach3_metrics['covered_topics'] > approach2_metrics['covered_topics'] else 'üèÜ Approach 2',\n",
    "    'üèÜ Approach 3' if approach3_metrics['avg_coverage'] > approach2_metrics['avg_coverage'] else 'üèÜ Approach 2',\n",
    "    'üèÜ Approach 3' if approach3_metrics['detail_score'] > approach2_metrics['detail_score'] else 'üèÜ Approach 2',\n",
    "    'üèÜ Approach 3' if approach3_metrics['detail_percentage'] > approach2_metrics['detail_percentage'] else 'üèÜ Approach 2',\n",
    "    'üèÜ Approach 3' if approach3_metrics['word_count'] > approach2_metrics['word_count'] else 'üèÜ Approach 2',\n",
    "    'üèÜ Approach 2',  # Faster is better\n",
    "    'üèÜ Approach 3' if approach3_metrics['word_count']/311.59 > approach2_metrics['word_count']/210.06 else 'üèÜ Approach 2'\n",
    "]\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 100)\n",
    "print(\"üéØ KEY INSIGHTS:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Calculate differences\n",
    "topic_diff = approach3_metrics['covered_topics'] - approach2_metrics['covered_topics']\n",
    "coverage_diff = approach3_metrics['avg_coverage'] - approach2_metrics['avg_coverage']\n",
    "detail_diff = approach3_metrics['detail_percentage'] - approach2_metrics['detail_percentage']\n",
    "word_diff = approach3_metrics['word_count'] - approach2_metrics['word_count']\n",
    "time_diff = 311.59 - 210.06\n",
    "\n",
    "print(f\"\\n1. üìä TOPIC COVERAGE:\")\n",
    "print(f\"   ‚Ä¢ Approach 3 covers {topic_diff} MORE topics than Approach 2\")\n",
    "print(f\"   ‚Ä¢ Approach 3 has {coverage_diff:.1f}% better keyword coverage across all topics\")\n",
    "\n",
    "print(f\"\\n2. üìù DETAIL LEVEL:\")\n",
    "print(f\"   ‚Ä¢ Approach 3 provides {detail_diff:.1f}% more detailed explanations\")\n",
    "print(f\"   ‚Ä¢ Approach 2 tends to just mention topics, Approach 3 explains them\")\n",
    "\n",
    "print(f\"\\n3. üìÑ COMPREHENSIVENESS:\")\n",
    "print(f\"   ‚Ä¢ Approach 3 produces {word_diff} more words ({(word_diff/approach2_metrics['word_count']*100):.1f}% increase)\")\n",
    "print(f\"   ‚Ä¢ This is not just verbosity - it covers MORE topics with MORE detail\")\n",
    "\n",
    "print(f\"\\n4. ‚è±Ô∏è  TIME TRADE-OFF:\")\n",
    "print(f\"   ‚Ä¢ Approach 3 takes {time_diff:.1f} seconds longer ({(time_diff/210.06*100):.1f}% increase)\")\n",
    "print(f\"   ‚Ä¢ But produces {(approach3_metrics['word_count']/approach2_metrics['word_count']):.1f}x more comprehensive output\")\n",
    "\n",
    "print(f\"\\n5. üéØ RECOMMENDATION:\")\n",
    "if approach3_metrics['covered_topics'] > approach2_metrics['covered_topics']:\n",
    "    print(f\"   ‚úÖ CHOOSE APPROACH 3 (Ensemble + Llama) because:\")\n",
    "    print(f\"      ‚Ä¢ Covers {(approach3_metrics['covered_topics']/approach3_metrics['total_topics']*100):.0f}% of all topics vs {(approach2_metrics['covered_topics']/approach2_metrics['total_topics']*100):.0f}% for Approach 2\")\n",
    "    print(f\"      ‚Ä¢ Provides {detail_diff:.1f}% more detailed explanations\")\n",
    "    print(f\"      ‚Ä¢ Worth the extra {time_diff:.1f}s for {(approach3_metrics['covered_topics']/approach2_metrics['covered_topics']-1)*100:.1f}% more topic coverage\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ CHOOSE APPROACH 2 (Hierarchical) because:\")\n",
    "    print(f\"      ‚Ä¢ Faster by {time_diff:.1f} seconds\")\n",
    "    print(f\"      ‚Ä¢ Similar topic coverage\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìä Analysis Complete! Above results show which approach better captures the original document's content.\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c26d41",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìÑ Complete Approach 3 Summary (Full Text)\n",
    "\n",
    "Below is the complete summary generated by **Approach 3 (Ensemble + Llama)**:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ceecb9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Approach 3 (Standalone Llama) vs Llama (Ensemble + Llama)\n",
    "\n",
    "This section compares:\n",
    "- **Approach 3:** Standalone Llama directly summarizing the document\n",
    "- **Llama:** Ensemble method (4 T5 models ‚Üí Llama merge)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dee2fd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ APPROACH 3: Standalone Llama (Direct Summarization)\n",
      "================================================================================\n",
      "Found Llama at: ./models\\llama3.2\\llama-3.2-3b-instruct-q4_k_m.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Llama loaded\n",
      "\n",
      "ü¶ô Generating standalone Llama summary...\n",
      "This may take 60-120 seconds...\n",
      "‚úÖ Standalone Llama complete\n",
      "‚úÖ Standalone Llama complete\n",
      "\n",
      "================================================================================\n",
      "üìã APPROACH 3 - STANDALONE LLAMA SUMMARY:\n",
      "================================================================================\n",
      "**Physical Layer Overview**\n",
      "\n",
      "The physical layer is a fundamental component of the cloud computing reference model, comprising physical compute, storage, and network resources. It is responsible for providing the infrastructure necessary for the execution of software applications and the storage of business and application data.\n",
      "\n",
      "**Compute System**\n",
      "\n",
      "A compute system is a computing platform that executes software applications and provides services to consumers. It consists of hardware, firmware, and software components, and is typically based on x86 architecture. Compute systems can be provided to consumers in two ways: shared hosting and dedicated hosting. Shared hosting involves multiple consumers sharing a single compute system, while dedicated hosting involves individual consumers having their own dedicated compute system.\n",
      "\n",
      "There are three main types of compute systems: tower, rack-mounted, and blade. Tower compute systems are built in an upright enclosure called a \"tower\" and have integrated power supply and cooling. Rack-mounted compute systems are designed to be fixed on a frame called a \"rack\" and simplify network cabling and reduce floor space use. Blade compute systems comprise an electronic circuit board with core processing components and are housed in a blade chassis.\n",
      "\n",
      "**Storage System**\n",
      "\n",
      "A storage system is the repository for saving and retrieving electronic data. It provides storage capacity along with compute systems, or as a service. Storage systems can be based on various architectures, including block-based, file-based, object-based, and unified. Block-based storage systems enable creating and assigning storage volumes to compute systems, while file-based storage systems involve a dedicated, high-performance file server with storage.\n",
      "\n",
      "There are several types of storage devices, including hard disk drives, solid-state drives, and tape drives. Redundant Array of Independent Disks (RAID) is a technique used to improve storage system performance by serving I/Os from multiple drives simultaneously. RAID techniques include striping, mirroring, and parity.\n",
      "\n",
      "**Network Connectivity**\n",
      "\n",
      "Networking enables data transfer and sharing of IT resources between nodes across geographic regions. It is essential for cloud consumers to access cloud services and data. There are several types of network communication, including compute-to-compute communication, compute-to-storage communication, and storage area network (SAN) classification.\n",
      "\n",
      "Inter-cloud communication involves connecting multiple data centers or multiple clouds to one another. This is essential for providing a reliable and secure network to cloud consumers. Network connectivity also enables the sharing of IT resources between nodes, such as compute systems and storage systems.\n",
      "\n",
      "**Cloud Computing Reference Model**\n",
      "\n",
      "The cloud computing reference model is a conceptual framework that describes the major components and interactions of cloud computing systems. It consists of five layers: physical layer, network layer, transport layer, session layer, and presentation layer.\n",
      "\n",
      "The physical layer is responsible for providing the infrastructure necessary for the execution of software applications and the storage of business and application data. The network layer enables data transfer and sharing of IT resources between nodes across geographic regions. The transport layer provides a reliable and secure connection between nodes. The session layer manages the interaction between nodes, while the presentation layer provides a user interface for accessing cloud services.\n",
      "\n",
      "**Key Concepts**\n",
      "\n",
      "* **Compute System**: A computing platform that executes software applications and provides services to consumers.\n",
      "* **Storage System**: The repository for saving and retrieving electronic data.\n",
      "* **Network Connectivity**: Enables data transfer and sharing of IT resources between nodes across geographic regions.\n",
      "* **RAID**: A technique used to improve storage system performance by serving I/Os from multiple drives simultaneously.\n",
      "* **Block-Based Storage System**: Enables creating and assigning storage volumes to compute systems.\n",
      "* **File-Based Storage System**: Involves a dedicated, high-performance file server with storage.\n",
      "* **Object-Based Storage System**: Stores file data in the form of objects based on data contents and attributes.\n",
      "* **Unified Storage System**: Combines multiple storage systems into a single system.\n",
      "* **Inter-Cloud Communication**: Connects multiple data centers or multiple clouds to one another.\n",
      "* **Network Layer**: Enables data transfer and sharing of IT resources between nodes across geographic regions.\n",
      "* **Transport Layer**: Provides a reliable and secure connection between nodes.\n",
      "* **Session Layer**: Manages the interaction between nodes.\n",
      "* **Presentation Layer**: Provides a user interface for accessing cloud services.\n",
      "\n",
      "**Technical Terms and Acronyms**\n",
      "\n",
      "* **x86**: A family of microprocessors developed by Intel and AMD.\n",
      "* **RAID**: Redundant Array of Independent Disks.\n",
      "* **NAS**: Network-Attached Storage.\n",
      "* **SAN**: Storage Area Network.\n",
      "* **Cloud Computing Reference Model**: A conceptual framework that describes the major components and interactions of cloud computing systems.\n",
      "* **Compute Virtualization**: A technique used to create virtual machines on a physical compute system.\n",
      "* **Virtual Machines**: Self-contained operating environments that run on a physical compute system.\n",
      "* **Block-Based Storage**: Enables creating and assigning storage volumes to compute systems.\n",
      "* **File-Based Storage**: Involves a dedicated, high-performance file server with storage.\n",
      "* **Object-Based Storage**: Stores file data in the form of objects based on data contents and attributes.\n",
      "* **Unified Storage**: Combines multiple storage systems into a single system.\n",
      "* **Inter-Cloud Communication**: Connects multiple data centers or multiple clouds to one another.\n",
      "* **Network Layer**: Enables data transfer and sharing of IT resources between nodes across geographic regions.\n",
      "* **Transport Layer**: Provides a reliable and secure connection between nodes.\n",
      "* **Session Layer**: Manages the interaction between nodes.\n",
      "* **Presentation Layer**: Provides a user interface for accessing cloud services.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "In conclusion, the physical layer is a fundamental component of the cloud computing reference model, comprising physical compute, storage, and network resources. It is responsible for providing the infrastructure necessary for the execution of software applications and the storage of business and application data. The compute system, storage system, and network connectivity are all essential components of the physical layer, and understanding these concepts is crucial for designing and implementing cloud computing systems.\n",
      "================================================================================\n",
      "\n",
      "üìä Total Word Count: 935 words\n",
      "‚è±Ô∏è  Total Time: 103.68s\n",
      "\n",
      "================================================================================\n",
      "üìã APPROACH 3 - STANDALONE LLAMA SUMMARY:\n",
      "================================================================================\n",
      "**Physical Layer Overview**\n",
      "\n",
      "The physical layer is a fundamental component of the cloud computing reference model, comprising physical compute, storage, and network resources. It is responsible for providing the infrastructure necessary for the execution of software applications and the storage of business and application data.\n",
      "\n",
      "**Compute System**\n",
      "\n",
      "A compute system is a computing platform that executes software applications and provides services to consumers. It consists of hardware, firmware, and software components, and is typically based on x86 architecture. Compute systems can be provided to consumers in two ways: shared hosting and dedicated hosting. Shared hosting involves multiple consumers sharing a single compute system, while dedicated hosting involves individual consumers having their own dedicated compute system.\n",
      "\n",
      "There are three main types of compute systems: tower, rack-mounted, and blade. Tower compute systems are built in an upright enclosure called a \"tower\" and have integrated power supply and cooling. Rack-mounted compute systems are designed to be fixed on a frame called a \"rack\" and simplify network cabling and reduce floor space use. Blade compute systems comprise an electronic circuit board with core processing components and are housed in a blade chassis.\n",
      "\n",
      "**Storage System**\n",
      "\n",
      "A storage system is the repository for saving and retrieving electronic data. It provides storage capacity along with compute systems, or as a service. Storage systems can be based on various architectures, including block-based, file-based, object-based, and unified. Block-based storage systems enable creating and assigning storage volumes to compute systems, while file-based storage systems involve a dedicated, high-performance file server with storage.\n",
      "\n",
      "There are several types of storage devices, including hard disk drives, solid-state drives, and tape drives. Redundant Array of Independent Disks (RAID) is a technique used to improve storage system performance by serving I/Os from multiple drives simultaneously. RAID techniques include striping, mirroring, and parity.\n",
      "\n",
      "**Network Connectivity**\n",
      "\n",
      "Networking enables data transfer and sharing of IT resources between nodes across geographic regions. It is essential for cloud consumers to access cloud services and data. There are several types of network communication, including compute-to-compute communication, compute-to-storage communication, and storage area network (SAN) classification.\n",
      "\n",
      "Inter-cloud communication involves connecting multiple data centers or multiple clouds to one another. This is essential for providing a reliable and secure network to cloud consumers. Network connectivity also enables the sharing of IT resources between nodes, such as compute systems and storage systems.\n",
      "\n",
      "**Cloud Computing Reference Model**\n",
      "\n",
      "The cloud computing reference model is a conceptual framework that describes the major components and interactions of cloud computing systems. It consists of five layers: physical layer, network layer, transport layer, session layer, and presentation layer.\n",
      "\n",
      "The physical layer is responsible for providing the infrastructure necessary for the execution of software applications and the storage of business and application data. The network layer enables data transfer and sharing of IT resources between nodes across geographic regions. The transport layer provides a reliable and secure connection between nodes. The session layer manages the interaction between nodes, while the presentation layer provides a user interface for accessing cloud services.\n",
      "\n",
      "**Key Concepts**\n",
      "\n",
      "* **Compute System**: A computing platform that executes software applications and provides services to consumers.\n",
      "* **Storage System**: The repository for saving and retrieving electronic data.\n",
      "* **Network Connectivity**: Enables data transfer and sharing of IT resources between nodes across geographic regions.\n",
      "* **RAID**: A technique used to improve storage system performance by serving I/Os from multiple drives simultaneously.\n",
      "* **Block-Based Storage System**: Enables creating and assigning storage volumes to compute systems.\n",
      "* **File-Based Storage System**: Involves a dedicated, high-performance file server with storage.\n",
      "* **Object-Based Storage System**: Stores file data in the form of objects based on data contents and attributes.\n",
      "* **Unified Storage System**: Combines multiple storage systems into a single system.\n",
      "* **Inter-Cloud Communication**: Connects multiple data centers or multiple clouds to one another.\n",
      "* **Network Layer**: Enables data transfer and sharing of IT resources between nodes across geographic regions.\n",
      "* **Transport Layer**: Provides a reliable and secure connection between nodes.\n",
      "* **Session Layer**: Manages the interaction between nodes.\n",
      "* **Presentation Layer**: Provides a user interface for accessing cloud services.\n",
      "\n",
      "**Technical Terms and Acronyms**\n",
      "\n",
      "* **x86**: A family of microprocessors developed by Intel and AMD.\n",
      "* **RAID**: Redundant Array of Independent Disks.\n",
      "* **NAS**: Network-Attached Storage.\n",
      "* **SAN**: Storage Area Network.\n",
      "* **Cloud Computing Reference Model**: A conceptual framework that describes the major components and interactions of cloud computing systems.\n",
      "* **Compute Virtualization**: A technique used to create virtual machines on a physical compute system.\n",
      "* **Virtual Machines**: Self-contained operating environments that run on a physical compute system.\n",
      "* **Block-Based Storage**: Enables creating and assigning storage volumes to compute systems.\n",
      "* **File-Based Storage**: Involves a dedicated, high-performance file server with storage.\n",
      "* **Object-Based Storage**: Stores file data in the form of objects based on data contents and attributes.\n",
      "* **Unified Storage**: Combines multiple storage systems into a single system.\n",
      "* **Inter-Cloud Communication**: Connects multiple data centers or multiple clouds to one another.\n",
      "* **Network Layer**: Enables data transfer and sharing of IT resources between nodes across geographic regions.\n",
      "* **Transport Layer**: Provides a reliable and secure connection between nodes.\n",
      "* **Session Layer**: Manages the interaction between nodes.\n",
      "* **Presentation Layer**: Provides a user interface for accessing cloud services.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "In conclusion, the physical layer is a fundamental component of the cloud computing reference model, comprising physical compute, storage, and network resources. It is responsible for providing the infrastructure necessary for the execution of software applications and the storage of business and application data. The compute system, storage system, and network connectivity are all essential components of the physical layer, and understanding these concepts is crucial for designing and implementing cloud computing systems.\n",
      "================================================================================\n",
      "\n",
      "üìä Total Word Count: 935 words\n",
      "‚è±Ô∏è  Total Time: 103.68s\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ APPROACH 3: Standalone Llama (Direct Summarization)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "standalone_start_time = time.time()\n",
    "\n",
    "# Find and load Llama model\n",
    "model_pattern = \"./models/**/llama-3.2-3b-instruct-q4_k_m.gguf\"\n",
    "model_files = glob.glob(model_pattern, recursive=True)\n",
    "\n",
    "if model_files:\n",
    "    llama_model_path = model_files[0]\n",
    "    print(f\"Found Llama at: {llama_model_path}\")\n",
    "    \n",
    "    llm = Llama(\n",
    "        model_path=llama_model_path,\n",
    "        n_ctx=4096,\n",
    "        n_threads=6,\n",
    "        n_gpu_layers=0,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Llama loaded\")\n",
    "    \n",
    "    # Create prompt for standalone Llama\n",
    "    standalone_prompt = f\"\"\"[INST]\n",
    "You are an expert technical summarization system. Create a comprehensive 500-700 word summary of the following document.\n",
    "\n",
    "Your summary must:\n",
    "1. Cover all major topics and concepts\n",
    "2. Preserve ALL technical terms, acronyms, and specific details exactly as written\n",
    "3. Maintain professional academic tone\n",
    "4. Organize information with clear section headings\n",
    "5. Provide detailed explanations for key concepts\n",
    "\n",
    "Document to summarize:\n",
    "{document_text[:6000]}\n",
    "\n",
    "Create the comprehensive summary:\n",
    "[/INST]\"\"\"\n",
    "    \n",
    "    # Generate with Llama\n",
    "    print(\"\\nü¶ô Generating standalone Llama summary...\")\n",
    "    print(\"This may take 60-120 seconds...\")\n",
    "    \n",
    "    output = llm(\n",
    "        standalone_prompt,\n",
    "        max_tokens=1800,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    standalone_llama_summary = output['choices'][0]['text'].strip()\n",
    "    standalone_total_time = time.time() - standalone_start_time\n",
    "    \n",
    "    print(f\"‚úÖ Standalone Llama complete\")\n",
    "    \n",
    "    del llm\n",
    "else:\n",
    "    print(\"‚ùå Llama model not found\")\n",
    "    standalone_llama_summary = \"[Llama model not available]\"\n",
    "    standalone_total_time = 0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã APPROACH 3 - STANDALONE LLAMA SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "print(standalone_llama_summary)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Total Word Count: {len(standalone_llama_summary.split())} words\")\n",
    "print(f\"‚è±Ô∏è  Total Time: {standalone_total_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b6a335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Analyzing Standalone Llama Summary...\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "üìä APPROACH 3: Standalone Llama - Topic Coverage Analysis\n",
      "====================================================================================================\n",
      "\n",
      "Topic                               Coverage        Detail Level              Sentences\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Physical Layer Overview             5/5 (100%)      ‚úÖ Detailed Explanation    43\n",
      "Compute Systems                     4/5 (80%)       ‚úÖ Detailed Explanation    22\n",
      "Storage Systems                     4/5 (80%)       ‚úÖ Detailed Explanation    14\n",
      "Network Connectivity                5/5 (100%)      ‚úÖ Detailed Explanation    18\n",
      "Compute System Components           1/5 (20%)       ‚úÖ Detailed Explanation    5\n",
      "Types of Compute Systems            3/4 (75%)       ‚úÖ Detailed Explanation    4\n",
      "Storage Architectures               2/5 (40%)       ‚úÖ Detailed Explanation    3\n",
      "Virtualization                      2/4 (50%)       ‚ö†Ô∏è  Moderate Detail       2\n",
      "\n",
      "====================================================================================================\n",
      "üìä Overall Metrics:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚úÖ Topics Covered: 8/8 (100.0%)\n",
      "üìà Average Keyword Coverage: 68.1%\n",
      "üìù Detail Score: 23.0/24 (95.8%)\n",
      "üìÑ Total Word Count: 935 words\n"
     ]
    }
   ],
   "source": [
    "# Analyze standalone Llama summary\n",
    "print(\"\\nüîç Analyzing Standalone Llama Summary...\")\n",
    "standalone_metrics = analyze_topic_coverage(standalone_llama_summary, \"APPROACH 3: Standalone Llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae7dacfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====================================================================================================\n",
      "üèÜ FINAL COMPARISON: Approach 3 vs Llama \n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "              Metric Approach 3   Llama        Winner\n",
      "      Topics Covered         8/8     6/8 üèÜ Approach 3\n",
      "    Topic Coverage %      100.0%   75.0% üèÜ Approach 3\n",
      "Avg Keyword Coverage       68.1%   51.9% üèÜ Approach 3\n",
      "        Detail Score     23.0/24 18.0/24 üèÜ Approach 3\n",
      "   Detail Percentage       95.8%   75.0% üèÜ Approach 3\n",
      "          Word Count         935     722 üèÜ Approach 3\n",
      "  Execution Time (s)      103.68  111.44 üèÜ Approach 3\n",
      "    Words per Second        9.02    6.48 üèÜ Approach 3\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "üéØ KEY INSIGHTS:\n",
      "====================================================================================================\n",
      "\n",
      "1. üìä TOPIC COVERAGE:\n",
      "   ‚Ä¢ Standalone Llama covers 2 MORE topics than Ensemble\n",
      "   ‚Ä¢ Standalone Llama has 16.2% better keyword coverage\n",
      "\n",
      "2. üìù DETAIL LEVEL:\n",
      "   ‚Ä¢ Standalone Llama provides 20.8% more detailed explanations\n",
      "   ‚Ä¢ Direct processing allows deeper focus on key concepts\n",
      "\n",
      "3. üìÑ COMPREHENSIVENESS:\n",
      "   ‚Ä¢ Standalone Llama produces 213 more words (29.5% increase)\n",
      "   ‚Ä¢ More concise while maintaining comprehensiveness\n",
      "\n",
      "4. ‚è±Ô∏è  TIME TRADE-OFF:\n",
      "   ‚Ä¢ Llama (Ensemble) takes 7.8 seconds longer (7.5% increase)\n",
      "   ‚Ä¢ Extra time spent on T5 extraction: 17.14s\n",
      "   ‚Ä¢ But produces 0.77x more comprehensive output\n",
      "\n",
      "5. üéØ RECOMMENDATION:\n",
      "   ‚úÖ CHOOSE APPROACH 3 (Standalone Llama) because:\n",
      "      ‚Ä¢ Faster by 7.8 seconds (7.0% reduction)\n",
      "      ‚Ä¢ Covers 100% of all topics\n",
      "      ‚Ä¢ Simpler pipeline (single model)\n",
      "      ‚Ä¢ Better for quick summaries or when T5 models unavailable\n",
      "\n",
      "====================================================================================================\n",
      "üìä Analysis Complete!\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\" + \"=\" * 100)\n",
    "print(\"üèÜ FINAL COMPARISON: Approach 3 vs Llama \")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create comparison DataFrame with the metrics from your image\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Topics Covered',\n",
    "        'Topic Coverage %',\n",
    "        'Avg Keyword Coverage',\n",
    "        'Detail Score',\n",
    "        'Detail Percentage',\n",
    "        'Word Count',\n",
    "        'Execution Time (s)',\n",
    "        'Words per Second'\n",
    "    ],\n",
    "    'Approach 3 ': [\n",
    "        f\"{standalone_metrics['covered_topics']}/{standalone_metrics['total_topics']}\",\n",
    "        f\"{standalone_metrics['covered_topics']/standalone_metrics['total_topics']*100:.1f}%\",\n",
    "        f\"{standalone_metrics['avg_coverage']:.1f}%\",\n",
    "        f\"{standalone_metrics['detail_score']:.1f}/{standalone_metrics['max_detail']}\",\n",
    "        f\"{standalone_metrics['detail_percentage']:.1f}%\",\n",
    "        standalone_metrics['word_count'],\n",
    "        f\"{standalone_total_time:.2f}\",\n",
    "        f\"{standalone_metrics['word_count']/standalone_total_time:.2f}\"\n",
    "    ],\n",
    "    'Llama ': [\n",
    "        f\"{approach3_metrics['covered_topics']}/{approach3_metrics['total_topics']}\",\n",
    "        f\"{approach3_metrics['covered_topics']/approach3_metrics['total_topics']*100:.1f}%\",\n",
    "        f\"{approach3_metrics['avg_coverage']:.1f}%\",\n",
    "        f\"{approach3_metrics['detail_score']:.1f}/{approach3_metrics['max_detail']}\",\n",
    "        f\"{approach3_metrics['detail_percentage']:.1f}%\",\n",
    "        approach3_metrics['word_count'],\n",
    "        f\"{approach3_total_time:.2f}\",\n",
    "        f\"{approach3_metrics['word_count']/approach3_total_time:.2f}\"\n",
    "    ],\n",
    "    'Winner': []\n",
    "}\n",
    "\n",
    "# Determine winners\n",
    "comparison_data['Winner'] = [\n",
    "    'üèÜ Llama' if approach3_metrics['covered_topics'] > standalone_metrics['covered_topics'] else 'üèÜ Approach 3',\n",
    "    'üèÜ Llama' if approach3_metrics['covered_topics'] > standalone_metrics['covered_topics'] else 'üèÜ Approach 3',\n",
    "    'üèÜ Llama' if approach3_metrics['avg_coverage'] > standalone_metrics['avg_coverage'] else 'üèÜ Approach 3',\n",
    "    'üèÜ Llama' if approach3_metrics['detail_score'] > standalone_metrics['detail_score'] else 'üèÜ Approach 3',\n",
    "    'üèÜ Llama' if approach3_metrics['detail_percentage'] > standalone_metrics['detail_percentage'] else 'üèÜ Approach 3',\n",
    "    'üèÜ Llama' if approach3_metrics['word_count'] > standalone_metrics['word_count'] else 'üèÜ Approach 3',\n",
    "    'üèÜ Approach 3' if standalone_total_time < approach3_total_time else 'üèÜ Llama',  # Faster is better\n",
    "    'üèÜ Llama' if approach3_metrics['word_count']/approach3_total_time > standalone_metrics['word_count']/standalone_total_time else 'üèÜ Approach 3'\n",
    "]\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 100)\n",
    "print(\"üéØ KEY INSIGHTS:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Calculate differences\n",
    "topic_diff = approach3_metrics['covered_topics'] - standalone_metrics['covered_topics']\n",
    "coverage_diff = approach3_metrics['avg_coverage'] - standalone_metrics['avg_coverage']\n",
    "detail_diff = approach3_metrics['detail_percentage'] - standalone_metrics['detail_percentage']\n",
    "word_diff = approach3_metrics['word_count'] - standalone_metrics['word_count']\n",
    "time_diff = approach3_total_time - standalone_total_time\n",
    "\n",
    "print(f\"\\n1. üìä TOPIC COVERAGE:\")\n",
    "if topic_diff > 0:\n",
    "    print(f\"   ‚Ä¢ Llama (Ensemble) covers {topic_diff} MORE topics than Standalone Llama\")\n",
    "    print(f\"   ‚Ä¢ Llama (Ensemble) has {coverage_diff:.1f}% better keyword coverage across all topics\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Standalone Llama covers {abs(topic_diff)} MORE topics than Ensemble\")\n",
    "    print(f\"   ‚Ä¢ Standalone Llama has {abs(coverage_diff):.1f}% better keyword coverage\")\n",
    "\n",
    "print(f\"\\n2. üìù DETAIL LEVEL:\")\n",
    "if detail_diff > 0:\n",
    "    print(f\"   ‚Ä¢ Llama (Ensemble) provides {detail_diff:.1f}% more detailed explanations\")\n",
    "    print(f\"   ‚Ä¢ T5 models extract diverse perspectives that Llama then expands\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Standalone Llama provides {abs(detail_diff):.1f}% more detailed explanations\")\n",
    "    print(f\"   ‚Ä¢ Direct processing allows deeper focus on key concepts\")\n",
    "\n",
    "print(f\"\\n3. üìÑ COMPREHENSIVENESS:\")\n",
    "if word_diff > 0:\n",
    "    print(f\"   ‚Ä¢ Llama (Ensemble) produces {word_diff} more words ({(word_diff/standalone_metrics['word_count']*100):.1f}% increase)\")\n",
    "    print(f\"   ‚Ä¢ This is not just verbosity - it covers MORE topics with MORE detail\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Standalone Llama produces {abs(word_diff)} more words ({(abs(word_diff)/approach3_metrics['word_count']*100):.1f}% increase)\")\n",
    "    print(f\"   ‚Ä¢ More concise while maintaining comprehensiveness\")\n",
    "\n",
    "print(f\"\\n4. ‚è±Ô∏è  TIME TRADE-OFF:\")\n",
    "if time_diff > 0:\n",
    "    print(f\"   ‚Ä¢ Llama (Ensemble) takes {time_diff:.1f} seconds longer ({(time_diff/standalone_total_time*100):.1f}% increase)\")\n",
    "    print(f\"   ‚Ä¢ Extra time spent on T5 extraction: {t5_extraction_time:.2f}s\")\n",
    "    print(f\"   ‚Ä¢ But produces {(approach3_metrics['word_count']/standalone_metrics['word_count']):.2f}x more comprehensive output\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Standalone Llama takes {abs(time_diff):.1f} seconds longer ({(abs(time_diff)/approach3_total_time*100):.1f}% increase)\")\n",
    "    print(f\"   ‚Ä¢ Skipping T5 extraction saves time but may miss diverse perspectives\")\n",
    "\n",
    "print(f\"\\n5. üéØ RECOMMENDATION:\")\n",
    "if approach3_metrics['covered_topics'] > standalone_metrics['covered_topics']:\n",
    "    print(f\"   ‚úÖ CHOOSE LLAMA (Ensemble Method) because:\")\n",
    "    print(f\"      ‚Ä¢ Covers {(approach3_metrics['covered_topics']/approach3_metrics['total_topics']*100):.0f}% of all topics vs {(standalone_metrics['covered_topics']/standalone_metrics['total_topics']*100):.0f}% for Standalone\")\n",
    "    print(f\"      ‚Ä¢ Provides {detail_diff:.1f}% more detailed explanations\")\n",
    "    print(f\"      ‚Ä¢ T5 models extract specialized perspectives (academic, news, conversational)\")\n",
    "    print(f\"      ‚Ä¢ Worth the extra {time_diff:.1f}s for {abs(topic_diff)} more topics covered\")\n",
    "    print(f\"      ‚Ä¢ Better for complex documents requiring comprehensive coverage\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ CHOOSE APPROACH 3 (Standalone Llama) because:\")\n",
    "    print(f\"      ‚Ä¢ Faster by {abs(time_diff):.1f} seconds ({(abs(time_diff)/approach3_total_time*100):.1f}% reduction)\")\n",
    "    print(f\"      ‚Ä¢ Covers {(standalone_metrics['covered_topics']/standalone_metrics['total_topics']*100):.0f}% of all topics\")\n",
    "    print(f\"      ‚Ä¢ Simpler pipeline (single model)\")\n",
    "    print(f\"      ‚Ä¢ Better for quick summaries or when T5 models unavailable\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìä Analysis Complete!\")\n",
    "print(\"=\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
