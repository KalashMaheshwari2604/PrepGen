{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nprint(\"ğŸ“¦ Installing packages and fixing compatibility issues...\")\n\n# Fix protobuf compatibility issue\n!pip uninstall -y protobuf\n!pip install -q protobuf==3.20.3\n\n# Install required packages\n!pip install -q transformers datasets accelerate sentencepiece evaluate rouge_score\n\nprint(\"\\nâœ… Installation complete!\")\nprint(\"âš ï¸  Ignore any dependency warnings - they won't affect training\")\nprint(\"ğŸ”„ If you see CUDA warnings above, that's normal - training will still work\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:05:35.930802Z","iopub.execute_input":"2025-11-08T16:05:35.930967Z","iopub.status.idle":"2025-11-08T16:07:18.054924Z","shell.execute_reply.started":"2025-11-08T16:05:35.930951Z","shell.execute_reply":"2025-11-08T16:07:18.053898Z"}},"outputs":[{"name":"stdout","text":"ğŸ“¦ Installing packages and fixing compatibility issues...\nFound existing installation: protobuf 6.33.0\nUninstalling protobuf-6.33.0:\n  Successfully uninstalled protobuf-6.33.0\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\nâœ… Installation complete!\nâš ï¸  Ignore any dependency warnings - they won't affect training\nğŸ”„ If you see CUDA warnings above, that's normal - training will still work\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =====================================================================\n# CELL 2: Imports & Configuration (Run Second, ~10 seconds)\n# =====================================================================\n\nimport os\nimport torch\nimport pandas as pd\nfrom datasets import load_dataset, concatenate_datasets, DatasetDict\nfrom transformers import (\n    T5ForConditionalGeneration, \n    T5Tokenizer,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer\n)\nimport evaluate\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport time\nimport json\nfrom collections import Counter\n\nprint(\"âœ… All imports successful!\")\n\n# Configuration\nCONFIG = {\n    # Model settings\n    \"model_name\": \"google/flan-t5-base\",\n    \"output_dir\": \"./academic-summarizer-scientific\",\n    \n    # Mixed Dataset Strategy (70-20-10 split)\n    \"scientific_samples\": 20000,  # 70% - arXiv scientific papers\n    \"booksum_samples\": 6000,      # 20% - Long-form book summaries\n    \"wikihow_samples\": 2500,      # 10% - Instructional articles\n    \n    # Training hyperparameters\n    \"learning_rate\": 3e-4,\n    \"num_epochs\": 3,\n    \"batch_size\": 8,\n    \"gradient_accumulation\": 4,\n    \n    # Text settings\n    \"max_input_length\": 1024,\n    \"max_target_length\": 512,\n    \n    # Optimization\n    \"fp16\": torch.cuda.is_available(),  # auto enable fp16 only if GPU\n    \"warmup_steps\": 500,\n}\nCONFIG[\"max_input_length\"] = 640\nCONFIG[\"max_target_length\"] = 160\n\nprint(\"\\nğŸš€ Configuration:\")\nprint(\"=\" * 60)\nprint(f\"Model: {CONFIG['model_name']}\")\nprint(f\"Dataset Mix: 70% Scientific + 20% BookSum + 10% WikiHow\")\nprint(f\"GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU Count: {torch.cuda.device_count()}\")\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\nprint(\"=\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:10:18.626028Z","iopub.execute_input":"2025-11-08T16:10:18.626742Z","iopub.status.idle":"2025-11-08T16:11:12.400058Z","shell.execute_reply.started":"2025-11-08T16:10:18.626716Z","shell.execute_reply":"2025-11-08T16:11:12.399057Z"}},"outputs":[{"name":"stderr","text":"2025-11-08 16:10:40.005199: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762618240.379367      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762618240.476270      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"âœ… All imports successful!\n\nğŸš€ Configuration:\n============================================================\nModel: google/flan-t5-base\nDataset Mix: 70% Scientific + 20% BookSum + 10% WikiHow\nGPU Available: True\nGPU Count: 2\nGPU Name: Tesla T4\n============================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# =====================================================================\n# CELL 3: Load Datasets (Run Third, ~5-10 minutes)\n# =====================================================================\n\nprint(\"\\nğŸ“¥ Loading datasets (this takes 5-10 minutes)...\\n\")\n\n# 1) Scientific Papers = ArXiv (Parquet-backed mirror)\nprint(\"1ï¸âƒ£ Loading Scientific Papers (ArXiv)...\")\nscientific_dataset = load_dataset(\n    \"ccdv/arxiv-summarization\",\n    \"document\",\n    split=f\"train[:{CONFIG['scientific_samples']}]\"\n)\nprint(f\"   âœ… Loaded {len(scientific_dataset):,} arXiv articles\")\n\n# 2) BookSum (auto-detect config)\nfrom datasets import get_dataset_config_names\ntry:\n    bs_configs = get_dataset_config_names(\"kmfoda/booksum\")\nexcept Exception:\n    bs_configs = [\"default\"]\nbs_config = bs_configs[0] if bs_configs else None\nprint(f\"\\n2ï¸âƒ£ Loading BookSum (config: {bs_config})...\")\nbooksum_dataset = load_dataset(\n    \"kmfoda/booksum\",\n    bs_config if bs_config else None,\n    split=f\"train[:{CONFIG['booksum_samples']}]\"\n)\nprint(f\"   âœ… Loaded {len(booksum_dataset):,} BookSum rows\")\nprint(\"   ğŸ§± BookSum columns:\", booksum_dataset.column_names)\n\n# 3) WikiHow (cleaned HF copy)\nprint(\"\\n3ï¸âƒ£ Loading WikiHow (cleaned)...\")\nwikihow_dataset = load_dataset(\n    \"gursi26/wikihow-cleaned\",\n    split=f\"train[:{CONFIG['wikihow_samples']}]\"\n)\nprint(f\"   âœ… Loaded {len(wikihow_dataset):,} WikiHow rows\")\nprint(\"   ğŸ§± WikiHow columns:\", wikihow_dataset.column_names)\n\nprint(\"\\nâœ… All datasets loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:13:04.799426Z","iopub.execute_input":"2025-11-08T16:13:04.800348Z","iopub.status.idle":"2025-11-08T16:14:28.761544Z","shell.execute_reply.started":"2025-11-08T16:13:04.800308Z","shell.execute_reply":"2025-11-08T16:14:28.760674Z"}},"outputs":[{"name":"stdout","text":"\nğŸ“¥ Loading datasets (this takes 5-10 minutes)...\n\n1ï¸âƒ£ Loading Scientific Papers (ArXiv)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"755b1df61c1c41f4903400fe2c126301"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00000-of-00015.parquet:   0%|          | 0.00/227M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfa384b849e54858aa6edfa86d6284b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00001-of-00015.parquet:   0%|          | 0.00/226M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cfe15d1a4a64be596756aa491074590"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00002-of-00015.parquet:   0%|          | 0.00/226M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ed35d5a466a4d03a6623d9b31b2e011"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00003-of-00015.parquet:   0%|          | 0.00/225M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ce87ed83c1c4ee2a372113a5f47f0b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00004-of-00015.parquet:   0%|          | 0.00/224M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90fa7601b15548119ea4484fefccd21e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00005-of-00015.parquet:   0%|          | 0.00/225M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb93f5aa18cb4e25ab052097e989db22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00006-of-00015.parquet:   0%|          | 0.00/226M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11fecd8aae8443fa9872541b4f2a445b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00007-of-00015.parquet:   0%|          | 0.00/228M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac9074f1ae5a4f71b5105525d72b0d55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00008-of-00015.parquet:   0%|          | 0.00/228M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"884735d30e99451f9febbbe00cfd18c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00009-of-00015.parquet:   0%|          | 0.00/226M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"677d2c725abd41c383e5b04fb61bd783"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00010-of-00015.parquet:   0%|          | 0.00/227M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4d0d4df67344d9bbb9056057cb66808"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00011-of-00015.parquet:   0%|          | 0.00/229M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6ebe8d9e0184ae7bb3d6167780eaa59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00012-of-00015.parquet:   0%|          | 0.00/228M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee79ec8a141e401eaf3a9b44d4958f91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00013-of-00015.parquet:   0%|          | 0.00/228M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce450b7c86684630b2d2cb4216240f1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/train-00014-of-00015.parquet:   0%|          | 0.00/233M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7e61439334541b89a9d1ab72a7bd54e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/validation-00000-of-00001.parqu(â€¦):   0%|          | 0.00/104M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdb0fc902717453fa638836dc95ae014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"document/test-00000-of-00001.parquet:   0%|          | 0.00/104M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71e2da1dc5fb47b9bfbe2e6f7244925f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/203037 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78b92f88474043adbe1fa880437b92a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/6436 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04a580f0fbeb4ddfbbdee9ba79f2e344"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/6440 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e029ef64dc174e7a8e44cdf8361dc5e4"}},"metadata":{}},{"name":"stdout","text":"   âœ… Loaded 20,000 arXiv articles\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3a426d2fb0b4177896812a6f64c95c9"}},"metadata":{}},{"name":"stdout","text":"\n2ï¸âƒ£ Loading BookSum (config: default)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/295M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"619f7490ee2f414a837cee88cebe684a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev.csv:   0%|          | 0.00/40.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ea023934cca463bb4978119581f3729"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/43.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"504a107b21fa44cbb0c7b09f922a7fd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8194ee00336a400eaa95aced85ab2488"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1484 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c8ae722ccde457fbf97a7aeb4aad8f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1431 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35d190fa22a244c5a61d431e650b6326"}},"metadata":{}},{"name":"stdout","text":"   âœ… Loaded 6,000 BookSum rows\n   ğŸ§± BookSum columns: ['bid', 'is_aggregate', 'source', 'chapter_path', 'summary_path', 'book_id', 'summary_id', 'content', 'summary', 'chapter', 'chapter_length', 'summary_name', 'summary_url', 'summary_text', 'summary_analysis', 'summary_length', 'analysis_length']\n\n3ï¸âƒ£ Loading WikiHow (cleaned)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/792 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d1b5fcd50b34bef949e6bff38609dea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikihow-cleaned.csv:   0%|          | 0.00/619M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f61249c1261843d8a1bc4c3ba86c9ba3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/214293 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73d1a982c2e04d17bfcb07dfef527665"}},"metadata":{}},{"name":"stdout","text":"   âœ… Loaded 2,500 WikiHow rows\n   ğŸ§± WikiHow columns: ['summary', 'title', 'text']\n\nâœ… All datasets loaded successfully!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# =====================================================================\n# CELL 4: Process & Combine Datasets (Run Fourth, ~2-3 minutes)\n# =====================================================================\n\nprint(\"\\nğŸ”„ Processing and combining datasets...\")\n\n# ---------------------------\n# Scientific (arXiv) â†’ uses: article, abstract\n# ---------------------------\ndef process_scientific(example):\n    return {\n        \"document\": example[\"article\"],\n        \"summary\": example[\"abstract\"],\n        \"source\": \"scientific\",\n    }\n\nscientific_processed = scientific_dataset.map(\n    process_scientific,\n    remove_columns=scientific_dataset.column_names,\n    desc=\"Processing Scientific Papers\",\n)\n\n# ---------------------------\n# BookSum â†’ columns seen: content, chapter, summary_text, summary\n# prefer 'chapter' if present, else 'content'; prefer 'summary_text' else 'summary'\n# ---------------------------\nBS_DOC_COL = \"chapter\" if \"chapter\" in booksum_dataset.column_names else \"content\"\nBS_SUM_COL = \"summary_text\" if \"summary_text\" in booksum_dataset.column_names else \"summary\"\n\ndef process_booksum(example):\n    return {\n        \"document\": example[BS_DOC_COL],\n        \"summary\": example[BS_SUM_COL],\n        \"source\": \"booksum\",\n    }\n\nbooksum_processed = booksum_dataset.map(\n    process_booksum,\n    remove_columns=booksum_dataset.column_names,\n    desc=\"Processing BookSum\",\n)\n\n# ---------------------------\n# WikiHow (cleaned) â†’ columns: text, summary, title\n# ---------------------------\ndef process_wikihow(example):\n    return {\n        \"document\": example[\"text\"],\n        \"summary\": example[\"summary\"],\n        \"source\": \"wikihow\",\n    }\n\nwikihow_processed = wikihow_dataset.map(\n    process_wikihow,\n    remove_columns=wikihow_dataset.column_names,\n    desc=\"Processing WikiHow\",\n)\n\n# ---------------------------\n# Combine â†’ shuffle â†’ split 90/10\n# ---------------------------\ncombined_dataset = concatenate_datasets(\n    [scientific_processed, booksum_processed, wikihow_processed]\n).shuffle(seed=42)\n\ndataset_split = combined_dataset.train_test_split(test_size=0.1, seed=42)\n\n# use 'test' split as validation\ndataset_dict = DatasetDict({\n    \"train\": dataset_split[\"train\"],\n    \"validation\": dataset_split[\"test\"],\n})\ndataset_dict[\"train\"] = dataset_dict[\"train\"].shuffle(seed=42).select(range(8000))\ndataset_dict[\"validation\"] = dataset_dict[\"validation\"].shuffle(seed=42).select(range(800))\n# ---------------------------\n# Show distribution\n# ---------------------------\nsources = dataset_dict[\"train\"][\"source\"]\nfrom collections import Counter  # in case cell order changes\ndistribution = Counter(sources)\n\nprint(\"\\nâœ… Datasets combined!\")\nprint(f\"Train: {len(dataset_dict['train']):,}\")\nprint(f\"Validation: {len(dataset_dict['validation']):,}\")\nprint(\"\\nğŸ“ˆ Training Distribution:\")\nfor source, count in distribution.items():\n    print(f\"  {source}: {count:,} ({count/len(sources)*100:.1f}%)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:19:28.350254Z","iopub.execute_input":"2025-11-08T16:19:28.350717Z","iopub.status.idle":"2025-11-08T16:19:34.661932Z","shell.execute_reply.started":"2025-11-08T16:19:28.350692Z","shell.execute_reply":"2025-11-08T16:19:34.660999Z"}},"outputs":[{"name":"stdout","text":"\nğŸ”„ Processing and combining datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Scientific Papers:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d36dad5a4be642f295e16d64b6af88e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing BookSum:   0%|          | 0/6000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb5c3a27ebf14efea2396faf22948bff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing WikiHow:   0%|          | 0/2500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e85935304ad486686404d89bd193056"}},"metadata":{}},{"name":"stdout","text":"\nâœ… Datasets combined!\nTrain: 8,000\nValidation: 800\n\nğŸ“ˆ Training Distribution:\n  scientific: 5,659 (70.7%)\n  booksum: 1,653 (20.7%)\n  wikihow: 688 (8.6%)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =====================================================================\n# CELL 5: Load Model & Tokenizer (Run Fifth, ~1 minute)\n# =====================================================================\n\nprint(\"\\nğŸ¤– Loading T5 model and tokenizer...\")\n\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\nmodel_name = CONFIG[\"model_name\"]\n\n# Tokenizer\ntokenizer = T5Tokenizer.from_pretrained(model_name)\n\n# Model\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# Device info\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\n\nprint(\"âœ… Model loaded!\")\nprint(f\"   Name: {model_name}\")\ntry:\n    print(f\"   Parameters: {model.num_parameters() / 1e6:.1f}M\")\nexcept Exception:\n    pass\n\nprint(f\"   Device: {device}\")\nif device == \"cuda\":\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:20:49.797924Z","iopub.execute_input":"2025-11-08T16:20:49.798727Z","iopub.status.idle":"2025-11-08T16:20:55.756850Z","shell.execute_reply.started":"2025-11-08T16:20:49.798701Z","shell.execute_reply":"2025-11-08T16:20:55.756011Z"}},"outputs":[{"name":"stdout","text":"\nğŸ¤– Loading T5 model and tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfa5913cbd894668a0bcf522b72cb077"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb42b27652e74829999bece3632b0dda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52b54909be994098ba160d2cb4c2f389"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bee23e014c38445ba09eaf642d48f720"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a20249ad12cb4ca99104a37ced095acd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3a418bbd02345e5a660a7f123f81f7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed1b6119b9484296b6dfa8543348cc66"}},"metadata":{}},{"name":"stdout","text":"âœ… Model loaded!\n   Name: google/flan-t5-base\n   Parameters: 247.6M\n   Device: cuda\n   GPU: Tesla T4\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# =====================================================================\n# CELL 6: Tokenize Datasets (Run Sixth, ~5-8 minutes)\n# =====================================================================\n\nprint(\"\\nğŸ”„ Tokenizing datasets (this takes 5-8 minutes)...\")\n\ndef preprocess_function(examples):\n    \n    # prefix instruction based on source\n    prefixes = []\n    for s in examples[\"source\"]:\n        if s == \"scientific\":\n            prefixes.append(\"summarize scientific paper: \")\n        elif s == \"booksum\":\n            prefixes.append(\"summarize book chapter: \")\n        else:\n            prefixes.append(\"summarize instructions: \")\n    \n    inputs = [p + d for p, d in zip(prefixes, examples[\"document\"])]\n    targets = examples[\"summary\"]\n\n    # tokenize input documents\n    model_inputs = tokenizer(\n        inputs,\n        max_length=CONFIG[\"max_input_length\"],\n        truncation=True,\n        padding=\"max_length\",\n    )\n\n    # tokenize summaries / labels\n    labels = tokenizer(\n        targets,\n        max_length=CONFIG[\"max_target_length\"],\n        truncation=True,\n        padding=\"max_length\",\n    )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n\ntokenized_dataset = dataset_dict.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=dataset_dict[\"train\"].column_names,\n    desc=\"Tokenizing\"\n)\n\nprint(\"âœ… Tokenization complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:21:35.737301Z","iopub.execute_input":"2025-11-08T16:21:35.737656Z","iopub.status.idle":"2025-11-08T16:25:50.573129Z","shell.execute_reply.started":"2025-11-08T16:21:35.737631Z","shell.execute_reply":"2025-11-08T16:25:50.572438Z"}},"outputs":[{"name":"stdout","text":"\nğŸ”„ Tokenizing datasets (this takes 5-8 minutes)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5b3d44c755a4653b88e4809d3660881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e5cd50d477c45329ad87457c61bb17f"}},"metadata":{}},{"name":"stdout","text":"âœ… Tokenization complete!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# =====================================================================\n# CELL 7C: Speed Mode (no re-tokenize) â€” finish within a short session\n# =====================================================================\nimport gc, torch\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n\nprint(\"âš¡ Switching to speed mode...\")\n\ngc.collect(); torch.cuda.empty_cache()\n\n# keep gradient checkpointing + no cache from 7B\ntry:\n    model.gradient_checkpointing_enable()\nexcept Exception:\n    pass\nmodel.config.use_cache = False\n\nspeed_args = Seq2SeqTrainingArguments(\n    output_dir=CONFIG[\"output_dir\"],\n    # âš ï¸ do fewer epochs\n    num_train_epochs=1,\n    optim=\"adafactor\",\n    # small per-device batch to avoid OOM, but large grad-accum to reduce steps/epoch\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=32,   # halves steps vs 16\n    learning_rate=CONFIG[\"learning_rate\"],\n    warmup_steps=200,\n    weight_decay=0.01,\n    fp16=CONFIG[\"fp16\"],\n    # ğŸš« turn off eval during training to save time\n    eval_strategy=\"no\",\n    predict_with_generate=False,      # generation is slow; skip during train\n    # avoid checkpoint overhead\n    save_steps=10_000_000,\n    save_total_limit=1,\n    logging_steps=200,\n    logging_dir=f\"{CONFIG['output_dir']}/logs\",\n    dataloader_num_workers=0,\n    dataloader_pin_memory=False,\n    remove_unused_columns=True,\n    report_to=\"none\",\n    group_by_length=True,\n)\n\nspeed_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, model=model, label_pad_token_id=-100, padding=True\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=speed_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],  # we won't evaluate during train\n    processing_class=tokenizer,   # replaces tokenizer=tokenizer (silences warning)\n    data_collator=speed_collator,\n    compute_metrics=None,         # metrics disabled during train for speed\n)\n\nprint(\"âœ… Speed mode ready. Now run Cell 8 again.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:26:05.062131Z","iopub.execute_input":"2025-11-08T16:26:05.062761Z","iopub.status.idle":"2025-11-08T16:26:05.550066Z","shell.execute_reply.started":"2025-11-08T16:26:05.062737Z","shell.execute_reply":"2025-11-08T16:26:05.549341Z"}},"outputs":[{"name":"stdout","text":"âš¡ Switching to speed mode...\nâœ… Speed mode ready. Now run Cell 8 again.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# =====================================================================\n# CELL 8: START TRAINING (Run Eighth â€” long)\n# =====================================================================\n\nimport math, time\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ğŸš€ STARTING TRAINING\")\nprint(\"=\" * 60)\n\n# Rough time estimate (purely informational)\ntotal_samples = len(tokenized_dataset[\"train\"])\neffective_batch = CONFIG[\"batch_size\"] * CONFIG[\"gradient_accumulation\"]\nsteps_per_epoch = max(1, math.floor(total_samples / effective_batch))\ntotal_steps = steps_per_epoch * CONFIG[\"num_epochs\"]\n\nprint(f\"Total train samples: {total_samples:,}\")\nprint(f\"Per-device batch size: {CONFIG['batch_size']}\")\nprint(f\"Gradient accumulation: {CONFIG['gradient_accumulation']}\")\nprint(f\"Effective batch size:  {effective_batch}\")\nprint(f\"Epochs: {CONFIG['num_epochs']}\")\nprint(f\"Estimated steps/epoch: {steps_per_epoch:,}\")\nprint(f\"Total steps (est.):    {total_steps:,}\")\nprint(\"=\" * 60 + \"\\n\")\n\nstart_time = time.time()\n\n# If you want to resume from the last checkpoint (if any), set this True\nRESUME_LAST = False\nif RESUME_LAST:\n    last_ckpt = trainer.state.best_model_checkpoint or None\n    trainer.train(resume_from_checkpoint=last_ckpt)\nelse:\n    trainer.train()\n\nend_time = time.time()\nelapsed = end_time - start_time\nh = int(elapsed // 3600)\nm = int((elapsed % 3600) // 60)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"âœ… TRAINING COMPLETE!\")\nprint(\"=\" * 60)\nprint(f\"â±ï¸  Wall-clock time: {h}h {m}m\")\nprint(\"=\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:26:13.916754Z","iopub.execute_input":"2025-11-08T16:26:13.917434Z","iopub.status.idle":"2025-11-08T20:02:48.065162Z","shell.execute_reply.started":"2025-11-08T16:26:13.917410Z","shell.execute_reply":"2025-11-08T20:02:48.064543Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nğŸš€ STARTING TRAINING\n============================================================\nTotal train samples: 8,000\nPer-device batch size: 8\nGradient accumulation: 4\nEffective batch size:  32\nEpochs: 3\nEstimated steps/epoch: 250\nTotal steps (est.):    750\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 3:32:58, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n============================================================\nâœ… TRAINING COMPLETE!\n============================================================\nâ±ï¸  Wall-clock time: 3h 36m\n============================================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# MINI FIX: re-define compute_metrics if kernel lost it\n\nimport numpy as np\nimport evaluate\nrouge = evaluate.load(\"rouge\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    pred_lens = [len(p.split()) for p in decoded_preds]\n    result[\"gen_len\"] = float(np.mean(pred_lens))\n\n    return {\n        \"rouge1\": result[\"rouge1\"],\n        \"rouge2\": result[\"rouge2\"],\n        \"rougeL\": result[\"rougeL\"],\n        \"avg_length\": result[\"gen_len\"],\n    }\nprint(\"âœ… compute_metrics restored.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:04:52.755208Z","iopub.execute_input":"2025-11-08T20:04:52.755830Z","iopub.status.idle":"2025-11-08T20:04:54.396258Z","shell.execute_reply.started":"2025-11-08T20:04:52.755790Z","shell.execute_reply":"2025-11-08T20:04:54.395365Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60f56261ec974456acd5987d678539ab"}},"metadata":{}},{"name":"stdout","text":"âœ… compute_metrics restored.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# =====================================================================\n# CELL 9: Evaluate Model (Run Ninth, ~2-3 minutes)\n# =====================================================================\n\nprint(\"\\nğŸ“Š Final Evaluation on Validation Set...\")\n\nimport gc, torch\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# If you used Speed Mode (Cell 7C), trainer may have compute_metrics=None and\n# predict_with_generate=False. For evaluation we rebuild a light eval-only trainer.\neval_args = Seq2SeqTrainingArguments(\n    output_dir=CONFIG[\"output_dir\"],\n    per_device_eval_batch_size=2,\n    dataloader_num_workers=0,\n    dataloader_pin_memory=False,\n    predict_with_generate=True,\n    generation_max_length=min(192, CONFIG[\"max_target_length\"]),\n    generation_num_beams=2,                 # keep it light on GPU\n    fp16=CONFIG[\"fp16\"],\n    report_to=\"none\",\n)\n\neval_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, model=model, label_pad_token_id=-100, padding=True\n)\n\neval_trainer = Seq2SeqTrainer(\n    model=model,\n    args=eval_args,\n    eval_dataset=tokenized_dataset[\"validation\"],\n    processing_class=tokenizer,             # (instead of tokenizer=...) silences warning\n    data_collator=eval_collator,\n    compute_metrics=compute_metrics,        # defined in Cell 7\n)\n\n# Run evaluation\neval_results = eval_trainer.evaluate()\n\nprint(\"\\nâœ… Evaluation Results:\")\nprint(\"-\" * 60)\nfor k in sorted(eval_results.keys()):\n    if isinstance(eval_results[k], (int, float)):\n        try:\n            print(f\"  {k}: {eval_results[k]:.4f}\")\n        except Exception:\n            print(f\"  {k}: {eval_results[k]}\")\n    else:\n        print(f\"  {k}: {eval_results[k]}\")\nprint(\"-\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:05:05.752000Z","iopub.execute_input":"2025-11-08T20:05:05.752760Z","iopub.status.idle":"2025-11-08T20:21:16.751222Z","shell.execute_reply.started":"2025-11-08T20:05:05.752736Z","shell.execute_reply":"2025-11-08T20:21:16.750343Z"}},"outputs":[{"name":"stdout","text":"\nğŸ“Š Final Evaluation on Validation Set...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 15:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nâœ… Evaluation Results:\n------------------------------------------------------------\n  eval_avg_length: 101.3700\n  eval_loss: 2.5207\n  eval_rouge1: 0.2933\n  eval_rouge2: 0.0754\n  eval_rougeL: 0.1989\n  eval_runtime: 970.5007\n  eval_samples_per_second: 0.8240\n  eval_steps_per_second: 0.2060\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# =====================================================================\n# CELL 10: Save & Quick Test (Run Tenth, ~2-3 minutes)\n# =====================================================================\n\nimport json, time, torch\n\nprint(\"\\nğŸ’¾ Saving final model & tokenizer...\")\n\noutput_path = \"./my_academic_summarizer_scientific\"\nmodel.save_pretrained(output_path)\ntokenizer.save_pretrained(output_path)\n\n# Collect training/eval info if available\ntry:\n    _final_metrics = {k: float(v) for k, v in eval_results.items()}\nexcept Exception:\n    _final_metrics = {}\n\ntraining_info = {\n    \"model\": CONFIG[\"model_name\"],\n    \"datasets\": {\n        \"scientific_papers\": int(CONFIG[\"scientific_samples\"]),\n        \"booksum\": int(CONFIG[\"booksum_samples\"]),\n        \"wikihow\": int(CONFIG[\"wikihow_samples\"]),\n        \"total\": int(CONFIG[\"scientific_samples\"] + CONFIG[\"booksum_samples\"] + CONFIG[\"wikihow_samples\"]),\n    },\n    \"final_metrics\": _final_metrics,\n    \"training_epochs\": int(getattr(trainer.args, \"num_train_epochs\", CONFIG[\"num_epochs\"])),\n    \"per_device_train_batch_size\": int(getattr(trainer.args, \"per_device_train_batch_size\", CONFIG[\"batch_size\"])),\n    \"gradient_accumulation_steps\": int(getattr(trainer.args, \"gradient_accumulation_steps\", CONFIG[\"gradient_accumulation\"])),\n    \"max_input_length\": int(CONFIG[\"max_input_length\"]),\n    \"max_target_length\": int(CONFIG[\"max_target_length\"]),\n    \"timestamp\": int(time.time()),\n}\n\nwith open(f\"{output_path}/training_info.json\", \"w\") as f:\n    json.dump(training_info, f, indent=2)\n\nprint(f\"âœ… Saved to: {output_path}\")\n\n# ---------------------------\n# Quick smoke test on 2 samples\n# ---------------------------\nprint(\"\\nğŸ§ª Quick generation test...\\n\")\n\ndevice = model.device if hasattr(model, \"device\") else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntest_examples = [\n    {\n        \"type\": \"Course Policy\",\n        \"text\": (\n            \"Cloud Computing (CC-702IT0C026) is designed for senior undergraduates. \"\n            \"It covers service models (IaaS, PaaS, SaaS), deployment models, virtualization, \"\n            \"and cloud security. Networks is a prerequisite.\"\n        ),\n    },\n    {\n        \"type\": \"Research Paper\",\n        \"text\": (\n            \"We propose federated learning for healthcare analytics, avoiding centralization \"\n            \"of patient data. Experiments show improved accuracy with differential privacy.\"\n        ),\n    },\n]\n\ngen_max = min(256, CONFIG[\"max_target_length\"])  # keep light for T4\n\nfor ex in test_examples:\n    print(\"=\" * 60)\n    print(f\"ğŸ“„ {ex['type']}\")\n    print(\"=\" * 60)\n\n    prompt = \"summarize scientific paper: \" + ex[\"text\"]\n\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        max_length=CONFIG[\"max_input_length\"],\n        truncation=True,\n    ).to(device)\n\n    with torch.no_grad():\n        summary_ids = model.generate(\n            **inputs,\n            max_length=gen_max,\n            num_beams=4,\n            length_penalty=1.0,\n            early_stopping=True,\n        )\n\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    print(f\"ğŸ“ Summary: {summary}\\n\")\n\nprint(\"âœ… Quick test complete. Ready for export.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:22:23.640833Z","iopub.execute_input":"2025-11-08T20:22:23.641655Z","iopub.status.idle":"2025-11-08T20:22:27.838746Z","shell.execute_reply.started":"2025-11-08T20:22:23.641623Z","shell.execute_reply":"2025-11-08T20:22:27.837997Z"}},"outputs":[{"name":"stdout","text":"\nğŸ’¾ Saving final model & tokenizer...\nâœ… Saved to: ./my_academic_summarizer_scientific\n\nğŸ§ª Quick generation test...\n\n============================================================\nğŸ“„ Course Policy\n============================================================\nğŸ“ Summary: Cloud Computing (CC-702IT0C026) is designed for senior undergraduates and covers service models (IaaS, PaaS, SaaS), deployment models, virtualization, and cloud security. Networks is a prerequisite.\n\n============================================================\nğŸ“„ Research Paper\n============================================================\nğŸ“ Summary: We propose federated learning for healthcare analytics avoiding centralization of patient data. Experiments show improved accuracy with differential privacy.\n\nâœ… Quick test complete. Ready for export.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# =====================================================================\n# CELL 11: Create Download ZIP (Run Last, ~1 minute)\n# =====================================================================\n\nzip_name = \"academic_summarizer_scientific.zip\"\nfolder_to_zip = \"./my_academic_summarizer_scientific\"\n\nprint(\"\\nğŸ“¦ Creating downloadable archive...\")\n\n!zip -r {zip_name} {folder_to_zip} > /dev/null\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ğŸ‰ FINE-TUNING COMPLETE!\")\nprint(\"=\" * 60)\nprint(f\"ğŸ“¦ Download file: {zip_name}\")\nprint(f\"ğŸ“ Model folder zipped: {folder_to_zip}\")\nprint(\"\\nğŸ‘‰ Go to Kaggle > left sidebar > Files > click the ZIP to download.\")\nprint(\"=\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:23:13.509186Z","iopub.execute_input":"2025-11-08T20:23:13.509468Z","iopub.status.idle":"2025-11-08T20:24:09.415790Z","shell.execute_reply.started":"2025-11-08T20:23:13.509451Z","shell.execute_reply":"2025-11-08T20:24:09.414758Z"}},"outputs":[{"name":"stdout","text":"\nğŸ“¦ Creating downloadable archive...\n\n============================================================\nğŸ‰ FINE-TUNING COMPLETE!\n============================================================\nğŸ“¦ Download file: academic_summarizer_scientific.zip\nğŸ“ Model folder zipped: ./my_academic_summarizer_scientific\n\nğŸ‘‰ Go to Kaggle > left sidebar > Files > click the ZIP to download.\n============================================================\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ==============================================================\n# DOWNLOAD LINK GENERATOR\n# ==============================================================\n\n# move file to working so UI can access\n!mv academic_summarizer_scientific.zip /kaggle/working/\n\nfrom IPython.display import FileLink\nprint(\"Click the link below to download:\")\nFileLink(\"/kaggle/working/academic_summarizer_scientific.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:27:40.292139Z","iopub.execute_input":"2025-11-08T20:27:40.292823Z","iopub.status.idle":"2025-11-08T20:27:40.529639Z","shell.execute_reply.started":"2025-11-08T20:27:40.292791Z","shell.execute_reply":"2025-11-08T20:27:40.528768Z"}},"outputs":[{"name":"stdout","text":"mv: 'academic_summarizer_scientific.zip' and '/kaggle/working/academic_summarizer_scientific.zip' are the same file\nClick the link below to download:\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/academic_summarizer_scientific.zip","text/html":"<a href='/kaggle/working/academic_summarizer_scientific.zip' target='_blank'>/kaggle/working/academic_summarizer_scientific.zip</a><br>"},"metadata":{}}],"execution_count":17}]}